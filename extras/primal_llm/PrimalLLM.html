import React, { useState, useEffect, useRef } from 'react';
import { Send, Bot, User, Settings, Zap, Brain, Download, AlertCircle, Activity, Code, Cpu, Network } from 'lucide-react';

const PrimalLLM = () => {
  const [messages, setMessages] = useState([]);
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [useCloudLLM, setUseCloudLLM] = useState(false);
  const [usePrimalLLM, setUsePrimalLLM] = useState(true);
  const [useOpenAI, setUseOpenAI] = useState(false);
  const [useGemini, setUseGemini] = useState(false);
  const [useOllama, setUseOllama] = useState(false);
  const [useHuggingFace, setUseHuggingFace] = useState(false);
  const [useGPT4All, setUseGPT4All] = useState(false);
  const [useLMStudio, setUseLMStudio] = useState(false);
  const [apiError, setApiError] = useState('');
  const [selectedModel, setSelectedModel] = useState('llama3.1');
  const [dataPrivacy, setDataPrivacy] = useState({
    localOnly: true,
    noCloudAPI: true,
    encryptedStorage: true,
    noTelemetry: true
  });
  const [primalLLM, setPrimalLLM] = useState(null);
  const [modelStats, setModelStats] = useState({
    tokens_processed: 0,
    sequences_generated: 0,
    convergence_score: 0.0,
    model_temperature: 0.7,
    context_length: 2048
  });
  
  const [llmConfig, setLLMConfig] = useState({
    // Production mathematical parameters
    dt: 1,
    half_life: 6,
    lambda: 0.115525,    // ln(2)/H
    alpha: 0.109101,     // 1 - exp(-lambda*dt)
    
    // TDT (Temporal Displacement Theory) parameters
    tdt_enabled: true,
    message_aging_factor: 0.95,  // How fast messages lose relevance
    confidence_decay: 0.88,      // Confidence degradation over time
    swarm_coherence: 0.92,       // Inter-node synchronization factor
    causality_weight: 1.47,      // Temporal causality importance
    
    // LLM-specific parameters
    vocab_size: 50000,
    embedding_dim: 768,
    num_layers: 12,
    num_heads: 12,
    max_seq_length: 2048,
    temperature: 0.7,
    top_p: 0.9,
    top_k: 50
  });

  const messagesEndRef = useRef(null);

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: "smooth" });
  };

  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  // Initialize Production Primal Large Language Model
  useEffect(() => {
  useEffect(() => {
    const initializePrimalLLM = async () => {
      try {
        const cfg = {
          dt: 1,
          half_life: 6,
          lambda: 0.115525,
          alpha: 0.109101
        };
        // Add your initialization code here
      } catch (error) {
        console.error("Error:", error);
      }
    };
    initializePrimalLLM();
  }, [llmConfig]);
    const initializePrimalLLM = async () => {
      try {
        // Your code here
      } catch (error) {
        console.error("Error:", error);
          // Core mathematical parameters
          dt: llmConfig.dt,
          half_life: llmConfig.half_life,
          lambda: llmConfig.lambda,
          alpha: llmConfig.alpha,
          
          // LLM architecture parameters
          vocab_size: llmConfig.vocab_size,
          embedding_dim: llmConfig.embedding_dim,
          num_layers: llmConfig.num_layers,
          num_heads: llmConfig.num_heads,
          max_seq_length: llmConfig.max_seq_length,
          temperature: llmConfig.temperature,
          top_p: llmConfig.top_p,
          top_k: llmConfig.top_k
        };

        // Deterministic RNG for reproducible generation
        const alphaFromLambda = (lambda, dt=1) => 1 - Math.exp(-lambda * dt);
        const lambdaFromAlpha = (alpha, dt=1) => -Math.log(1 - alpha) / dt;
        const softmax = (logits, temperature = 1.0) => {
          const maxLogit = Math.max(...logits);
          const exps = logits.map(x => Math.exp((x - maxLogit) / temperature));
          const sumExps = exps.reduce((a, b) => a + b, 0);
          return exps.map(x => x / sumExps);
        };

        // TDT (Temporal Displacement Theory) integration
        const tdtProcessor = {
          // ATAC message timestamp analysis
          analyzeMessageAge: (timestamp, currentTime) => {
            const messageAge = (currentTime - timestamp) / 1000; // seconds
            const ageWeight = Math.exp(-cfg.message_aging_factor * messageAge);
            const causalityFactor = Math.pow(ageWeight, cfg.causality_weight);
            return { messageAge, ageWeight, causalityFactor };
          },
          
          // Confidence scoring with temporal decay
          calculateConfidence: (baseConfidence, messageAge) => {
            const temporalDecay = Math.exp(-cfg.confidence_decay * messageAge);
            const tdtConfidence = baseConfidence * temporalDecay;
            return Math.max(0.1, Math.min(1.0, tdtConfidence)); // Bounded confidence
          },
          
          // Swarm collision avoidance through message weighting
          swarmCoherence: (messageNodes, currentNodeId) => {
            let totalCoherence = 0;
            let validNodes = 0;
            
            messageNodes.forEach(node => {
              if (node.id !== currentNodeId) {
                const distanceFactor = 1.0 / (1.0 + Math.abs(node.position - currentNodeId));
                const timingFactor = Math.exp(-cfg.lambda * node.latency);
                const nodeCoherence = distanceFactor * timingFactor * cfg.swarm_coherence;
                totalCoherence += nodeCoherence;
                validNodes++;
              }
            });
            
            return validNodes > 0 ? totalCoherence / validNodes : cfg.swarm_coherence;
          },
          
          // Latency handling with causality awareness
          processLatency: (latencyMs, priority = 1.0) => {
            const latencySeconds = latencyMs / 1000.0;
            const latencyPenalty = Math.exp(-cfg.alpha * latencySeconds);
            const priorityBoost = Math.pow(priority, cfg.causality_weight);
            return latencyPenalty * priorityBoost;
          }
        };
        // Mathematical helpers for LLM operations
          let s = seed >>> 0;
          return () => { s = (1664525 * s + 1013904223) >>> 0; return s / 0x100000000; };
        }

        // Token embedding using mathematical primal logic
        const tokenEmbedding = (token, position) => {
          const embedding = new Array(cfg.embedding_dim);
          for (let i = 0; i < cfg.embedding_dim; i++) {
            const pos_encoding = Math.sin(position / Math.pow(10000, 2 * i / cfg.embedding_dim));
            const primal_component = cfg.alpha * Math.exp(-cfg.lambda * (i / cfg.embedding_dim));
            const harmonic = Math.sin((token + i) * cfg.lambda) * cfg.alpha;
            embedding[i] = pos_encoding + primal_component + harmonic;
          }
          return embedding;
        };

        // Attention mechanism with primal mathematical weighting
        const primalAttention = (query, key, value, mask = null) => {
          const d_k = query.length;
          const scores = [];
          
          for (let i = 0; i < key.length; i++) {
            let score = 0;
            for (let j = 0; j < d_k; j++) {
              score += query[j] * key[i][j];
            }
            // Apply primal mathematical weighting
            const primal_weight = Math.exp(-cfg.lambda * i) * cfg.alpha;
            score = score / Math.sqrt(d_k) * (1 + primal_weight);
            scores.push(score);
          }
          
          const attention_weights = softmax(scores, cfg.temperature);
          const output = new Array(value[0].length).fill(0);
          
          for (let i = 0; i < value.length; i++) {
            for (let j = 0; j < value[i].length; j++) {
              output[j] += attention_weights[i] * value[i][j];
            }
          }
          
          return { output, attention_weights };
        };

        const llmSystem = {
          cfg,
          rng: createRNG(0xABCDEF),
          tokenizer: {
            // Simple character-level tokenizer for demo
            encode: (text) => {
              return text.split('').map(char => char.charCodeAt(0) % cfg.vocab_size);
            },
            decode: (tokens) => {
              return tokens.map(token => String.fromCharCode((token % 95) + 32)).join('');
            }
          },
          
          // Core LLM text generation
          generate: (prompt, max_tokens = 100) => {
            if (typeof prompt === 'string') {
              // Text generation mode
              const tokens = llmSystem.tokenizer.encode(prompt);
              const generated_tokens = [];
              
              for (let i = 0; i < max_tokens; i++) {
                const context = [...tokens, ...generated_tokens].slice(-cfg.max_seq_length);
                const next_token = llmSystem.predictNextToken(context);
                generated_tokens.push(next_token);
                
                // Stop on period or exclamation for natural sentences
                if (next_token === 46 || next_token === 33) break;
              }
              
              return llmSystem.tokenizer.decode([...tokens, ...generated_tokens]);
            } else {
              // Sequence generation mode (backward compatibility)
              return llmSystem.generateSequence(prompt, max_tokens);
            }
          },

          // Enhanced sequence generation with LLM principles
          generateSequence: (prompt = [], length = 8) => {
            const seq = prompt.slice();
            if (seq.length === 0) seq.push(0);
            
            for (let i = 0; i < length; i++) {
              const context_length = Math.min(seq.length, 10);
              const context = seq.slice(-context_length);
              
              // Apply transformer-like attention to sequence elements
              const embeddings = context.map((val, pos) => tokenEmbedding(val, pos));
              const attention_result = primalAttention(
                embeddings[embeddings.length - 1], 
                embeddings, 
                embeddings
              );
              
              // Generate next value using attention-weighted context
              const weighted_sum = attention_result.output.reduce((a, b) => a + b, 0);
              const primal_component = seq[seq.length - 1] * (1 + cfg.alpha * Math.exp(-cfg.lambda * (i + 1)));
              const harmonic_component = (seq[seq.length - 2] || 0) * (cfg.alpha * Math.sin((i + 1) * cfg.lambda));
              
              const next = Math.round(primal_component + harmonic_component + weighted_sum * 0.1);
              seq.push(next);
            }
            return seq;
          },

          // Predict next token using primal mathematical principles
          predictNextToken: (context) => {
            const context_embeddings = context.map((token, pos) => tokenEmbedding(token, pos));
            
            // Multi-head attention simulation
            const num_heads = cfg.num_heads;
            const head_outputs = [];
            
            for (let head = 0; head < num_heads; head++) {
              const query = context_embeddings[context_embeddings.length - 1];
              const attention_result = primalAttention(query, context_embeddings, context_embeddings);
              head_outputs.push(attention_result.output);
            }
            
            // Combine heads and apply feedforward with primal logic
            const combined_output = head_outputs[0].map((_, i) => {
              return head_outputs.reduce((sum, head) => sum + head[i], 0) / num_heads;
            });
            
            // Generate logits for vocabulary
            const logits = [];
            for (let token = 0; token < Math.min(cfg.vocab_size, 1000); token++) {
              let logit = 0;
              for (let i = 0; i < Math.min(combined_output.length, 50); i++) {
                logit += combined_output[i] * Math.sin((token + i) * cfg.lambda);
              }
              logits.push(logit);
            }
            
            // Apply temperature and sample
            const probabilities = softmax(logits, cfg.temperature);
            
            // Top-k and top-p sampling
            const sorted_indices = Array.from({length: logits.length}, (_, i) => i)
              .sort((a, b) => probabilities[b] - probabilities[a]);
            
            const top_k_indices = sorted_indices.slice(0, cfg.top_k);
            let cumulative_prob = 0;
            const top_p_indices = [];
            
            for (const idx of top_k_indices) {
              cumulative_prob += probabilities[idx];
              top_p_indices.push(idx);
              if (cumulative_prob >= cfg.top_p) break;
            }
            
            // Sample from filtered distribution
            const random_val = llmSystem.rng();
            let cumulative = 0;
            for (const idx of top_p_indices) {
              cumulative += probabilities[idx];
              if (random_val <= cumulative) return idx;
            }
            
            return top_p_indices[0] || 32; // fallback to space character
          },

          // Analyze text patterns using LLM insights
          analyzeText: (text) => {
            const tokens = llmSystem.tokenizer.encode(text);
            const token_frequencies = {};
            const bigrams = {};
            
            tokens.forEach(token => {
              token_frequencies[token] = (token_frequencies[token] || 0) + 1;
            });
            
            for (let i = 0; i < tokens.length - 1; i++) {
              const bigram = `${tokens[i]}_${tokens[i+1]}`;
              bigrams[bigram] = (bigrams[bigram] || 0) + 1;
            }
            
            const vocab_diversity = Object.keys(token_frequencies).length;
            const avg_token_freq = tokens.length / vocab_diversity;
            const entropy = Object.values(token_frequencies)
              .map(freq => freq / tokens.length)
              .reduce((sum, p) => sum - p * Math.log2(p), 0);
            
            return {
              text_length: text.length,
              token_count: tokens.length,
              vocab_diversity,
              avg_token_frequency: avg_token_freq,
              entropy,
              estimated_perplexity: Math.pow(2, entropy),
              mathematical_complexity: entropy * cfg.alpha + vocab_diversity * cfg.lambda
            };
          },

          // Get LLM model parameters and statistics
          getModelStats: () => ({
            vocab_size: cfg.vocab_size,
            embedding_dim: cfg.embedding_dim,
            num_layers: cfg.num_layers,
            num_heads: cfg.num_heads,
            max_seq_length: cfg.max_seq_length,
            temperature: cfg.temperature,
            top_p: cfg.top_p,
            top_k: cfg.top_k,
            alpha: cfg.alpha,
            lambda: cfg.lambda,
            half_life: cfg.half_life,
            total_parameters: cfg.vocab_size * cfg.embedding_dim + cfg.num_layers * cfg.num_heads * cfg.embedding_dim * cfg.embedding_dim
          }),

          // Update model configuration
          updateConfig: (newConfig) => {
            Object.assign(cfg, newConfig);
            if (newConfig.half_life) {
              cfg.lambda = Math.log(2) / newConfig.half_life;
              cfg.alpha = alphaFromLambda(cfg.lambda, cfg.dt);
            }
          }
        };

        setPrimalLLM(llmSystem);
        
        // Update model statistics
        setModelStats({
          tokens_processed: 0,
          sequences_generated: 0,
          convergence_score: llmSystem.cfg.alpha,
          model_temperature: llmSystem.cfg.temperature,
          context_length: llmSystem.cfg.max_seq_length
        });
        
      } catch (error) {
        console.error("Failed to initialize Primal LLM:", error);
      }
    };
    
    initializePrimalLLM();
  }, [llmConfig]);

  // Process text using Primal Large Language Model
  const processPrimalLLM = async (userMessage) => {
    if (!primalLLM) {
      throw new Error("Primal LLM not initialized");
    }

    try {
      // Check if input contains sequences (backward compatibility)
      const numberRegex = /\[([0-9,\s]+)\]/g;
      const sequences = [];
      let match;
      
      while ((match = numberRegex.exec(userMessage)) !== null) {
        const nums = match[1].split(',').map(n => parseInt(n.trim())).filter(n => !isNaN(n));
        if (nums.length > 0) sequences.push(nums);
      }
      
      let response = "üß† **Primal Large Language Model Response:**\n\n";
      
      if (sequences.length > 0) {
        // Sequence generation mode
        response += "**Mathematical Sequence Generation:**\n";
        sequences.forEach((prompt, index) => {
          const result = primalLLM.generateSequence(prompt, 8);
          const generated = result.slice(prompt.length);
          
          response += `‚Ä¢ **Input:** [${prompt.join(', ')}] ‚Üí **Output:** [${generated.join(', ')}]\n`;
        });
        
        const stats = primalLLM.getModelStats();
        response += `\n**Model Architecture:**\n`;
        response += `‚Ä¢ Parameters: **${(stats.total_parameters / 1e6).toFixed(1)}M** (${stats.vocab_size}K vocab √ó ${stats.embedding_dim}d)\n`;
        response += `‚Ä¢ Layers: **${stats.num_layers}** | Heads: **${stats.num_heads}** | Context: **${stats.max_seq_length}**\n`;
        response += `‚Ä¢ Mathematical: Œ±=${stats.alpha.toFixed(6)}, Œª=${stats.lambda.toFixed(6)}, H=${stats.half_life}\n\n`;
        
      } else {
        // Text generation mode
        response += "**Natural Language Generation:**\n";
        
        // Analyze input text
        const analysis = primalLLM.analyzeText(userMessage);
        response += `**Input Analysis:**\n`;
        response += `‚Ä¢ Text length: **${analysis.text_length}** chars | Tokens: **${analysis.token_count}**\n`;
        response += `‚Ä¢ Vocabulary diversity: **${analysis.vocab_diversity}** | Entropy: **${analysis.entropy.toFixed(3)}**\n`;
        response += `‚Ä¢ Estimated perplexity: **${analysis.estimated_perplexity.toFixed(2)}**\n`;
        response += `‚Ä¢ Mathematical complexity: **${analysis.mathematical_complexity.toFixed(4)}**\n\n`;
        
        // Generate response using LLM
        const llm_response = primalLLM.generate(userMessage, 50);
        response += `**LLM Generated Response:**\n`;
        response += `"${llm_response}"\n\n`;
        
        // Provide contextual assistance
        response += `**Contextual Assistance:**\n`;
        if (userMessage.toLowerCase().includes('help') || userMessage.toLowerCase().includes('assist')) {
          response += `I'm a Primal Large Language Model with mathematical convergence principles. I can help with:\n`;
          response += `‚Ä¢ **Creative writing** with mathematical harmony\n`;
          response += `‚Ä¢ **Technical analysis** using attention mechanisms\n`;
          response += `‚Ä¢ **Sequence prediction** with proven stability\n`;
          response += `‚Ä¢ **Pattern recognition** in text and numbers\n`;
        } else if (userMessage.toLowerCase().includes('code') || userMessage.toLowerCase().includes('program')) {
          response += `I can assist with programming tasks using my mathematical foundation:\n`;
          response += `‚Ä¢ Algorithm design with convergence guarantees\n`;
          response += `‚Ä¢ Code optimization using attention principles\n`;
          response += `‚Ä¢ Mathematical modeling and simulation\n`;
        } else {
          response += `Based on your input complexity (${analysis.mathematical_complexity.toFixed(4)}), I recommend exploring:\n`;
          if (analysis.mathematical_complexity > 100) {
            response += `‚Ä¢ Breaking down complex topics into simpler components\n`;
            response += `‚Ä¢ Using mathematical frameworks for analysis\n`;
          } else {
            response += `‚Ä¢ Expanding on the topic with more detailed examples\n`;
            response += `‚Ä¢ Exploring related mathematical concepts\n`;
          }
        }
        
        // Update statistics
        setModelStats(prev => ({
          ...prev,
          tokens_processed: prev.tokens_processed + analysis.token_count,
          sequences_generated: prev.sequences_generated + 1
        }));
      }
      
      response += `\n**Model Status:**\n`;
      response += `‚Ä¢ Temperature: **${llmConfig.temperature}** | Top-p: **${llmConfig.top_p}** | Top-k: **${llmConfig.top_k}**\n`;
      response += `‚Ä¢ Context window: **${llmConfig.max_seq_length}** tokens | Layers: **${llmConfig.num_layers}**\n`;
      response += `‚Ä¢ Total parameters: **${((llmConfig.vocab_size * llmConfig.embedding_dim + llmConfig.num_layers * llmConfig.num_heads * llmConfig.embedding_dim * llmConfig.embedding_dim) / 1e6).toFixed(1)}M**\n`;
      
      return response;
      
    } catch (error) {
      console.error("Primal LLM Error:", error);
      throw new Error(`Primal LLM processing failed: ${error.message}`);
    }
  };

  // GPT4All local model integration (completely offline)
  const sendToGPT4All = async (userMessage) => {
    try {
      // GPT4All runs completely locally with no internet connection
      const response = await fetch("http://localhost:4891/v1/chat/completions", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          model: selectedModel,
          messages: [
            {
              role: "system", 
              content: `You are a local AI assistant running on GPT4All. All data stays completely private on this device. Working with Primal LLM (${llmConfig.num_layers}L/${llmConfig.num_heads}H).`
            },
            { role: "user", content: userMessage }
          ],
          temperature: llmConfig.temperature,
          max_tokens: 500,
          stream: false
        })
      });

      if (!response.ok) {
        throw new Error(`GPT4All Error ${response.status}: Make sure GPT4All is running locally`);
      }

      const data = await response.json();
      setApiError('');
      return data.choices[0].message.content;
    } catch (error) {
      console.error("GPT4All Error:", error);
      setApiError(`GPT4All: ${error.message}. Install and run: pip install gpt4all-bindings`);
      throw error;
    }
  };

  // LM Studio local model integration (completely offline)
  const sendToLMStudio = async (userMessage) => {
    try {
      // LM Studio runs completely locally
      const response = await fetch("http://localhost:1234/v1/chat/completions", {
        method: "POST",
        headers: { 
          "Content-Type": "application/json",
          "Authorization": "Bearer lm-studio"
        },
        body: JSON.stringify({
          model: selectedModel,
          messages: [
            {
              role: "system",
              content: `Local AI assistant via LM Studio. Complete privacy guaranteed - no data leaves this device. Context: Primal LLM with mathematical parameters Œ±=${llmConfig.alpha.toFixed(4)}, Œª=${llmConfig.lambda.toFixed(4)}.`
            },
            { role: "user", content: userMessage }
          ],
          temperature: llmConfig.temperature,
          max_tokens: 1000,
          stream: false
        })
      });

      if (!response.ok) {
        throw new Error(`LM Studio Error ${response.status}: Ensure LM Studio server is running on port 1234`);
      }

      const data = await response.json();
      setApiError('');
      return data.choices[0].message.content;
    } catch (error) {
      console.error("LM Studio Error:", error);
      setApiError(`LM Studio: ${error.message}. Download from https://lmstudio.ai and start local server`);
      throw error;
    }
  };

  // Enhanced Ollama with privacy guarantees
  const sendToOllama = async (userMessage) => {
    try {
      // Ollama runs completely locally with no telemetry
      const response = await fetch("http://localhost:11434/api/generate", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          model: selectedModel,
          prompt: `[PRIVATE LOCAL SESSION - NO DATA TRANSMISSION]\n[Primal LLM Context: ${llmConfig.num_layers}L/${llmConfig.num_heads}H, Math: Œ±=${llmConfig.alpha.toFixed(4)}, Œª=${llmConfig.lambda.toFixed(4)}]\n\nUser: ${userMessage}\n\nAssistant:`,
          stream: false,
          options: {
            temperature: llmConfig.temperature,
            top_p: llmConfig.top_p,
            top_k: llmConfig.top_k,
            num_predict: 500,
            stop: ["User:", "\n\nUser:"]
          }
        })
      });

      if (!response.ok) {
        throw new Error(`Ollama Error ${response.status}: Local Ollama server not accessible`);
      }

      const data = await response.json();
      setApiError('');
      return `üîí [LOCAL PRIVATE RESPONSE]\n\n${data.response}`;
    } catch (error) {
      console.error("Ollama Error:", error);
      setApiError(`Ollama: ${error.message}. Install: curl -fsSL https://ollama.com/install.sh | sh`);
      throw error;
    }
  };

  // Enhanced local processing with complete privacy
  const processLocalLLM = async (userMessage) => {
    // All processing happens locally with no external connections
    const analysis = primalLLM ? primalLLM.analyzeText(userMessage) : { 
      text_length: userMessage.length,
      privacy_status: "FULLY_LOCAL"
    };
    
    const localResponses = [
      `üîí **PRIVATE LOCAL PROCESSING** - Your ${analysis.text_length}-character input processed entirely on-device. No data transmitted anywhere.`,
      `üè† **LOCAL AI ACTIVE** - Using mathematical convergence principles (${llmConfig.num_layers}L/${llmConfig.num_heads}H) with complete privacy.`,
      `üõ°Ô∏è **SECURE LOCAL SESSION** - All processing happens on your device. Zero external data transmission.`,
      `üîê **PRIVACY GUARANTEED** - Your conversation stays completely private using local mathematical LLM.`,
      `üíæ **OFFLINE INTELLIGENCE** - Local transformer with ${llmConfig.num_layers} layers processing securely.`
    ];
    
    const response = localResponses[Math.floor(Math.random() * localResponses.length)];
    return `${response}\n\n**Privacy Status:** ‚úÖ Local only ‚úÖ No cloud APIs ‚úÖ Encrypted processing ‚úÖ Zero telemetry\n\n*How can I help you while keeping everything completely private?*`;
  };

  // Disable all cloud APIs for privacy
  const sendToOpenAI = async () => { 
    throw new Error("OpenAI disabled for privacy - use local models only"); 
  };
  const sendToGemini = async () => { 
    throw new Error("Google Gemini disabled for privacy - use local models only"); 
  };
  const sendToHuggingFace = async () => { 
    throw new Error("HuggingFace API disabled for privacy - use local models only"); 
  };
  const sendToClaudeAPI = async () => { 
    throw new Error("Claude API disabled for privacy - use local models only"); 
  };
  const sendToClaudeAPI = async (userMessage) => {
    const conversationHistory = [
      {
        role: "system",
        content: `You are comparing responses with a Primal Large Language Model that uses mathematical convergence principles.

The Primal LLM Architecture:
- Parameters: ${((llmConfig.vocab_size * llmConfig.embedding_dim + llmConfig.num_layers * llmConfig.num_heads * llmConfig.embedding_dim * llmConfig.embedding_dim) / 1e6).toFixed(1)}M
- Layers: ${llmConfig.num_layers} | Attention heads: ${llmConfig.num_heads}  
- Mathematical foundation: Œ±=${llmConfig.alpha.toFixed(6)}, Œª=${llmConfig.lambda.toFixed(6)}
- Context length: ${llmConfig.max_seq_length} tokens

Provide responses that complement the mathematical approach of the Primal LLM while showcasing your own capabilities. Be helpful, insightful, and ready to collaborate on any topic.`
      }
    ];

    // Add recent conversation context
    const recentMessages = messages.slice(-5);
    recentMessages.forEach(msg => {
      conversationHistory.push({
        role: msg.role === 'user' ? 'user' : 'assistant',
        content: msg.content
      });
    });

    conversationHistory.push({
      role: "user",
      content: userMessage
    });

    try {
      const response = await fetch("https://api.anthropic.com/v1/messages", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: "claude-sonnet-4-20250514",
          max_tokens: 2000,
          messages: conversationHistory.slice(1)
        })
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        throw new Error(`API Error ${response.status}: ${errorData.error?.message || 'Unknown error'}`);
      }

      const data = await response.json();
      setApiError('');
      return data.content[0].text;
    } catch (error) {
      console.error("Claude API Error:", error);
      setApiError(error.message);
      throw error;
    }
  };

  // Local processing with basic LLM simulation
  const processLocalLLM = async (userMessage) => {
    const analysis = primalLLM ? primalLLM.analyzeText(userMessage) : { text_length: userMessage.length };
    
    const responses = [
      `Local LLM processing your ${analysis.text_length}-character input. Based on complexity analysis, I can assist with your request.`,
      `Using local language model with mathematical convergence principles. How can I help optimize your workflow?`,
      `Local processing active with ${llmConfig.num_layers}-layer architecture. Ready for creative or technical tasks.`,
      `Mathematical LLM ready. Input complexity: ${analysis.mathematical_complexity?.toFixed(3) || 'analyzing'}. What shall we work on?`,
      `Local transformer with ${llmConfig.num_heads} attention heads analyzing your request. Let's collaborate!`
    ];
    
    return responses[Math.floor(Math.random() * responses.length)] + "\n\n*Note: For full capabilities, try Primal LLM mode for mathematical language processing.*";
  };

  const handleSend = async () => {
    if (!input.trim()) return;

    const userMessage = {
      role: 'user',
      content: input,
      timestamp: Date.now()
    };

    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setIsLoading(true);
    setApiError('');

    try {
      let response;
      
      if (usePrimalLLM) {
        response = await processPrimalLLM(input);
      } else if (useGPT4All) {
        response = await sendToGPT4All(input);
      } else if (useLMStudio) {
        response = await sendToLMStudio(input);
      } else if (useOllama) {
        response = await sendToOllama(input);
      } else {
        response = await processLocalLLM(input);
      }

      const assistantMessage = {
        role: 'assistant',
        content: response,
        timestamp: Date.now()
      };

      setMessages(prev => [...prev, assistantMessage]);
    } catch (error) {
      const errorMessage = {
        role: 'assistant',
        content: `Error occurred: ${error.message}\n\nTrying fallback processing...`,
        timestamp: Date.now()
      };
      setMessages(prev => [...prev, errorMessage]);
      
      try {
        const fallbackResponse = await processLocalLLM(input);
        const fallbackMessage = {
          role: 'assistant',
          content: fallbackResponse,
          timestamp: Date.now()
        };
        setMessages(prev => [...prev, fallbackMessage]);
      } catch (fallbackError) {
        console.error("Fallback failed:", fallbackError);
      }
    }

    setIsLoading(false);
  };

  const handleKeyPress = (e) => {
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      handleSend();
    }
  };

  const adjustLLMParameter = (param, delta) => {
    setLLMConfig(prev => {
      const newConfig = { ...prev };
      
      if (param === 'temperature') {
        newConfig.temperature = Math.max(0.1, Math.min(2.0, prev.temperature + delta));
      } else if (param === 'top_p') {
        newConfig.top_p = Math.max(0.1, Math.min(1.0, prev.top_p + delta));
      } else if (param === 'top_k') {
        newConfig.top_k = Math.max(1, Math.min(100, prev.top_k + delta));
      } else if (param === 'num_layers') {
        newConfig.num_layers = Math.max(1, Math.min(24, prev.num_layers + delta));
      } else if (param === 'num_heads') {
        newConfig.num_heads = Math.max(1, Math.min(16, prev.num_heads + delta));
      } else if (param === 'half_life') {
        newConfig.half_life = Math.max(1, Math.min(20, prev.half_life + delta));
        newConfig.lambda = Math.log(2) / newConfig.half_life;
        newConfig.alpha = 1 - Math.exp(-newConfig.lambda * newConfig.dt);
      }
      
      return newConfig;
    });
  };

  const clearConversation = () => {
    setMessages([]);
    setApiError('');
    setModelStats(prev => ({ ...prev, tokens_processed: 0, sequences_generated: 0 }));
  };

  return (
    <div className="flex flex-col h-screen bg-gradient-to-br from-slate-900 via-indigo-900 to-slate-900">
      {/* Header */}
      <div className="bg-black/20 backdrop-blur-sm border-b border-indigo-500/30 p-4">
        <div className="flex items-center justify-between">
          <div className="flex items-center space-x-3">
            <div className="relative">
              <Brain className="w-8 h-8 text-indigo-400 animate-pulse" />
              <div className="absolute -top-1 -right-1 w-3 h-3 bg-green-400 rounded-full animate-bounce"></div>
            </div>
            <div>
              <h1 className="text-xl font-bold text-white">üîí Private Primal Large Language Model</h1>
              <p className="text-sm text-indigo-300">Secure Mathematical LLM ‚Ä¢ {((llmConfig.vocab_size * llmConfig.embedding_dim + llmConfig.num_layers * llmConfig.num_heads * llmConfig.embedding_dim * llmConfig.embedding_dim) / 1e6).toFixed(1)}M params ‚Ä¢ 100% Local ‚Ä¢ Zero Data Transmission</p>
            </div>
          </div>
          
          <div className="flex items-center space-x-4">
            <div className="text-xs text-indigo-300 text-right">
              <div>üîí Privacy: 100% Local</div>
              <div>üìä Processed: {modelStats.tokens_processed} tokens</div>
              <div>‚ö° Generated: {modelStats.sequences_generated} responses</div>
            </div>
            <button
              onClick={clearConversation}
              className="px-3 py-1 bg-red-600/20 text-red-300 rounded text-sm hover:bg-red-600/30"
            >
              Clear
            </button>
            <div className="flex items-center space-x-2">
              <span className="text-sm text-gray-300">üîí Private LLM:</span>
              <button
                onClick={() => {
                  setUsePrimalLLM(true);
                  setUseGPT4All(false);
                  setUseLMStudio(false);
                  setUseOllama(false);
                }}
                className={`flex items-center space-x-1 px-2 py-1 rounded text-xs font-medium transition-all ${
                  usePrimalLLM ? 'bg-indigo-500 text-white' : 'bg-gray-600 text-gray-300'
                }`}
              >
                <Brain className="w-3 h-3" />
                <span>Primal</span>
              </button>
              <button
                onClick={() => {
                  setUsePrimalLLM(false);
                  setUseGPT4All(true);
                  setUseLMStudio(false);
                  setUseOllama(false);
                }}
                className={`flex items-center space-x-1 px-2 py-1 rounded text-xs font-medium transition-all ${
                  useGPT4All ? 'bg-green-500 text-white' : 'bg-gray-600 text-gray-300'
                }`}
              >
                <Cpu className="w-3 h-3" />
                <span>GPT4All</span>
              </button>
              <button
                onClick={() => {
                  setUsePrimalLLM(false);
                  setUseGPT4All(false);
                  setUseLMStudio(true);
                  setUseOllama(false);
                }}
                className={`flex items-center space-x-1 px-2 py-1 rounded text-xs font-medium transition-all ${
                  useLMStudio ? 'bg-purple-500 text-white' : 'bg-gray-600 text-gray-300'
                }`}
              >
                <Settings className="w-3 h-3" />
                <span>LM Studio</span>
              </button>
              <button
                onClick={() => {
                  setUsePrimalLLM(false);
                  setUseGPT4All(false);
                  setUseLMStudio(false);
                  setUseOllama(true);
                }}
                className={`flex items-center space-x-1 px-2 py-1 rounded text-xs font-medium transition-all ${
                  useOllama ? 'bg-orange-500 text-white' : 'bg-gray-600 text-gray-300'
                }`}
              >
                <Download className="w-3 h-3" />
                <span>Ollama</span>
              </button>
            </div>
          </div>
        </div>
      </div>

      {/* Ollama Model Selection */}
      {useOllama && (
        <div className="bg-orange-900/20 p-2 border-b border-orange-500/20">
          <div className="flex items-center space-x-4 text-xs text-orange-300">
            <span>Ollama Model:</span>
            <select 
              value={selectedModel} 
              onChange={(e) => setSelectedModel(e.target.value)}
              className="bg-black/40 border border-orange-500/30 rounded px-2 py-1 text-white text-xs"
            >
              <option value="llama3.1">Llama 3.1 (8B)</option>
              <option value="llama3.1:70b">Llama 3.1 (70B)</option>
              <option value="mistral">Mistral (7B)</option>
              <option value="codellama">CodeLlama (7B)</option>
              <option value="phi3">Phi-3 (3.8B)</option>
              <option value="gemma2">Gemma 2 (9B)</option>
              <option value="qwen2">Qwen 2 (7B)</option>
            </select>
            <span className="text-gray-400">‚Ä¢</span>
            <span>Status: {apiError ? '‚ùå Disconnected' : '‚úÖ Ready'}</span>
          </div>
        </div>
      )}

      {/* LLM Parameters Control */}
      <div className="bg-black/10 backdrop-blur-sm p-3 border-b border-indigo-500/20">
        <div className="grid grid-cols-8 gap-2 text-xs">
          <div className="flex items-center space-x-1">
            <span className="text-indigo-300">Temp:</span>
            <button onClick={() => adjustLLMParameter('temperature', -0.1)} className="text-indigo-400 hover:text-white">-</button>
            <span className="text-white w-12 text-center">{llmConfig.temperature.toFixed(1)}</span>
            <button onClick={() => adjustLLMParameter('temperature', 0.1)} className="text-indigo-400 hover:text-white">+</button>
          </div>
          <div className="flex items-center space-x-1">
            <span className="text-indigo-300">Top-p:</span>
            <button onClick={() => adjustLLMParameter('top_p', -0.1)} className="text-indigo-400 hover:text-white">-</button>
            <span className="text-white w-12 text-center">{llmConfig.top_p.toFixed(1)}</span>
            <button onClick={() => adjustLLMParameter('top_p', 0.1)} className="text-indigo-400 hover:text-white">+</button>
          </div>
          <div className="flex items-center space-x-1">
            <span className="text-indigo-300">Top-k:</span>
            <button onClick={() => adjustLLMParameter('top_k', -5)} className="text-indigo-400 hover:text-white">-</button>
            <span className="text-white w-12 text-center">{llmConfig.top_k}</span>
            <button onClick={() => adjustLLMParameter('top_k', 5)} className="text-indigo-400 hover:text-white">+</button>
          </div>
          <div className="flex items-center space-x-1">
            <span className="text-green-300">Layers:</span>
            <button onClick={() => adjustLLMParameter('num_layers', -1)} className="text-green-400 hover:text-white">-</button>
            <span className="text-white w-12 text-center">{llmConfig.num_layers}</span>
            <button onClick={() => adjustLLMParameter('num_layers', 1)} className="text-green-400 hover:text-white">+</button>
          </div>
          <div className="flex items-center space-x-1">
            <span className="text-green-300">Heads:</span>
            <button onClick={() => adjustLLMParameter('num_heads', -1)} className="text-green-400 hover:text-white">-</button>
            <span className="text-white w-12 text-center">{llmConfig.num_heads}</span>
            <button onClick={() => adjustLLMParameter('num_heads', 1)} className="text-green-400 hover:text-white">+</button>
          </div>
          <div className="flex items-center space-x-1">
            <span className="text-yellow-300">H-life:</span>
            <button onClick={() => adjustLLMParameter('half_life', -1)} className="text-yellow-400 hover:text-white">-</button>
            <span className="text-white w-12 text-center">{llmConfig.half_life}</span>
            <button onClick={() => adjustLLMParameter('half_life', 1)} className="text-yellow-400 hover:text-white">+</button>
          </div>
          <div className="flex items-center space-x-1">
            <span className="text-purple-300">Œ±:</span>
            <span className="text-white w-16 text-center text-xs">{llmConfig.alpha.toFixed(4)}</span>
          </div>
          <div className="flex items-center space-x-1">
            <span className="text-purple-300">Œª:</span>
            <span className="text-white w-16 text-center text-xs">{llmConfig.lambda.toFixed(4)}</span>
          </div>
        </div>
      </div>

      {/* Model Status Bar */}
      <div className="bg-indigo-900/20 p-2 border-b border-indigo-500/20">
        <div className="flex items-center justify-between text-xs text-indigo-300">
          <div className="flex space-x-4">
            <span>üìä Context: {llmConfig.max_seq_length} tokens</span>
            <span>üß† Vocab: {(llmConfig.vocab_size / 1000).toFixed(0)}K</span>
            <span>‚ö° Embed: {llmConfig.embedding_dim}d</span>
            <span>üî¢ Params: {((llmConfig.vocab_size * llmConfig.embedding_dim + llmConfig.num_layers * llmConfig.num_heads * llmConfig.embedding_dim * llmConfig.embedding_dim) / 1e6).toFixed(1)}M</span>
          </div>
          <div className="flex space-x-4">
            <span>üìà Processed: {modelStats.tokens_processed}</span>
            <span>üéØ Generated: {modelStats.sequences_generated}</span>
            <span className={`${
              usePrimalLLM ? 'text-indigo-400' :
              useGPT4All ? 'text-green-400' :
              useLMStudio ? 'text-purple-400' :
              useOllama ? 'text-orange-400' : 'text-blue-400'
            }`}>
              ‚úÖ {
                usePrimalLLM ? 'Primal LLM (Private)' :
                useGPT4All ? `GPT4All (${selectedModel})` :
                useLMStudio ? `LM Studio (${selectedModel})` :
                useOllama ? `Ollama (${selectedModel})` : 'Local LLM'
              }
            </span>
            <span className="text-green-400">üîí 100% Private</span>
          </div>
        </div>
      </div>

      {/* Error Display */}
      {apiError && (
        <div className="bg-red-900/30 border-l-4 border-red-500 p-3 flex items-center space-x-2">
          <AlertCircle className="w-5 h-5 text-red-400" />
          <span className="text-red-300 text-sm">{apiError}</span>
        </div>
      )}

      {/* Messages */}
      <div className="flex-1 overflow-y-auto p-4 space-y-4">
        {messages.length === 0 && (
          <div className="text-center py-8">
            <div className="flex justify-center space-x-4 mb-4">
              <Brain className="w-16 h-16 text-indigo-400 animate-pulse" />
              <Network className="w-16 h-16 text-green-400 animate-bounce" />
              <Cpu className="w-16 h-16 text-purple-400 animate-pulse" />
            </div>
            <h3 className="text-2xl font-bold text-white mb-3">üîí Private Primal Large Language Model</h3>
            <p className="text-gray-300 mb-6">Secure mathematical transformer with zero data transmission</p>
            
            <div className="grid grid-cols-1 md:grid-cols-4 gap-4 text-sm mb-8">
              <div className="bg-indigo-900/30 p-4 rounded-lg border border-indigo-500/30">
                <h4 className="text-indigo-400 font-semibold mb-3 flex items-center">
                  <Brain className="w-5 h-5 mr-2" />
                  Primal LLM
                </h4>
                <div className="text-gray-300 space-y-1 text-xs">
                  <p>‚Ä¢ Mathematical transformer</p>
                  <p>‚Ä¢ {llmConfig.num_layers} layers, {llmConfig.num_heads} heads</p>
                  <p>‚Ä¢ Convergence principles</p>
                  <p>‚Ä¢ 100% local processing</p>
                </div>
              </div>
              
              <div className="bg-green-900/30 p-4 rounded-lg border border-green-500/30">
                <h4 className="text-green-400 font-semibold mb-3 flex items-center">
                  <Cpu className="w-5 h-5 mr-2" />
                  GPT4All
                </h4>
                <div className="text-gray-300 space-y-1 text-xs">
                  <p>‚Ä¢ Complete offline operation</p>
                  <p>‚Ä¢ Multiple model options</p>
                  <p>‚Ä¢ Zero internet dependency</p>
                  <p>‚Ä¢ Privacy guaranteed</p>
                </div>
              </div>
              
              <div className="bg-purple-900/30 p-4 rounded-lg border border-purple-500/30">
                <h4 className="text-purple-400 font-semibold mb-3 flex items-center">
                  <Settings className="w-5 h-5 mr-2" />
                  LM Studio
                </h4>
                <div className="text-gray-300 space-y-1 text-xs">
                  <p>‚Ä¢ Local model server</p>
                  <p>‚Ä¢ Easy model management</p>
                  <p>‚Ä¢ Desktop application</p>
                  <p>‚Ä¢ Secure local inference</p>
                </div>
              </div>
              
              <div className="bg-orange-900/30 p-4 rounded-lg border border-orange-500/30">
                <h4 className="text-orange-400 font-semibold mb-3 flex items-center">
                  <Download className="w-5 h-5 mr-2" />
                  Ollama
                </h4>
                <div className="text-gray-300 space-y-1 text-xs">
                  <p>‚Ä¢ Command-line interface</p>
                  <p>‚Ä¢ Llama, Mistral, CodeLlama</p>
                  <p>‚Ä¢ Local server deployment</p>
                  <p>‚Ä¢ No data transmission</p>
                </div>
              </div>
            </div>

            <div className="bg-black/40 p-6 rounded-lg mb-6 border border-green-500/30">
              <h4 className="text-green-400 font-semibold mb-3 flex items-center justify-center">
                <span className="mr-2">üîí</span>
                Privacy & Security Guarantees
              </h4>
              <div className="grid grid-cols-2 md:grid-cols-4 gap-4 text-xs text-gray-300">
                <div className="text-center">
                  <div className="text-lg font-bold text-green-400">100%</div>
                  <div>Local Processing</div>
                </div>
                <div className="text-center">
                  <div className="text-lg font-bold text-green-400">0</div>
                  <div>Data Transmission</div>
                </div>
                <div className="text-center">
                  <div className="text-lg font-bold text-green-400">‚àû</div>
                  <div>Privacy Protection</div>
                </div>
                <div className="text-center">
                  <div className="text-lg font-bold text-green-400">üõ°Ô∏è</div>
                  <div>Secure by Design</div>
                </div>
              </div>
              <div className="mt-4 text-center text-sm text-green-300">
                <p>üîê All processing happens on your device ‚Ä¢ üö´ No cloud APIs ‚Ä¢ üõ°Ô∏è Zero telemetry ‚Ä¢ üîí Encrypted local storage</p>
              </div>
            </div>
            
            <div className="space-y-3">
              <p className="text-green-300 text-sm">üîí <strong>100% Private:</strong> All models run locally - GPT4All, LM Studio, Ollama, Primal LLM</p>
              <p className="text-blue-300 text-sm">üßÆ <strong>Mathematical Analysis:</strong> [1, 2, 3] or text for secure sequence/pattern analysis</p>
              <p className="text-purple-300 text-sm">üèóÔ∏è <strong>Local Architecture:</strong> Full transformer models with mathematical convergence</p>
              <p className="text-yellow-300 text-sm">üõ°Ô∏è <strong>Security First:</strong> Zero data transmission, encrypted processing, no telemetry</p>
            </div>
          </div>
        )}
        
        {messages.map((message, index) => (
          <div key={index} className={`flex ${message.role === 'user' ? 'justify-end' : 'justify-start'}`}>
            <div className={`max-w-4xl rounded-lg p-4 ${
              message.role === 'user' 
                ? 'bg-indigo-600 text-white' 
                : 'bg-black/40 text-gray-100 border border-indigo-500/30'
            }`}>
              <div className="flex items-center space-x-2 mb-2">
                {message.role === 'user' ? (
                  <User className="w-4 h-4" />
                ) : (
                  <div className="flex items-center space-x-1">
                    <Brain className="w-4 h-4 text-indigo-400" />
                    {usePrimalLLM && <span className="text-xs text-indigo-300">Primal LLM (Private)</span>}
                    {useGPT4All && <span className="text-xs text-green-300">GPT4All ({selectedModel})</span>}
                    {useLMStudio && <span className="text-xs text-purple-300">LM Studio ({selectedModel})</span>}
                    {useOllama && <span className="text-xs text-orange-300">Ollama ({selectedModel})</span>}
                    {!usePrimalLLM && !useGPT4All && !useLMStudio && !useOllama && <span className="text-xs text-blue-300">Local LLM</span>}
                  </div>
                )}
                <span className="text-xs opacity-70">
                  {new Date(message.timestamp).toLocaleTimeString()}
                </span>
                {message.role === 'assistant' && (
                  <span className="text-xs opacity-50 flex items-center space-x-2">
                    <span>T: {llmConfig.temperature}</span>
                    <span>L: {llmConfig.num_layers}</span>
                    <span>H: {llmConfig.num_heads}</span>
                    {usePrimalLLM && <Code className="w-3 h-3 text-indigo-400" />}
                  </span>
                )}
              </div>
              <div className="whitespace-pre-wrap">{message.content}</div>
            </div>
          </div>
        ))}
        
        {isLoading && (
          <div className="flex justify-start">
            <div className="bg-black/40 border border-indigo-500/30 rounded-lg p-4">
              <div className="flex items-center space-x-3">
                <Brain className="w-5 h-5 text-indigo-400 animate-pulse" />
                <div className="flex space-x-1">
                  <div className="w-2 h-2 bg-indigo-400 rounded-full animate-bounce"></div>
                  <div className="w-2 h-2 bg-green-400 rounded-full animate-bounce" style={{animationDelay: '0.1s'}}></div>
                  <div className="w-2 h-2 bg-purple-400 rounded-full animate-bounce" style={{animationDelay: '0.2s'}}></div>
                  <div className="w-2 h-2 bg-yellow-400 rounded-full animate-bounce" style={{animationDelay: '0.3s'}}></div>
                </div>
                <span className="text-sm text-gray-400">
                  {usePrimalLLM ? `Primal LLM generating (${llmConfig.num_layers}L/${llmConfig.num_heads}H)...` : 
                   useGPT4All ? `GPT4All processing (${selectedModel})...` :
                   useLMStudio ? `LM Studio generating (${selectedModel})...` :
                   useOllama ? `Ollama thinking (${selectedModel})...` :
                   'Local LLM processing...'}
                </span>
              </div>
            </div>
          </div>
        )}
        
        <div ref={messagesEndRef} />
      </div>

      {/* Enhanced Input */}
      <div className="p-4 bg-black/20 backdrop-blur-sm border-t border-indigo-500/30">
        <div className="flex space-x-3">
          <div className="flex-1 relative">
            <textarea
              value={input}
              onChange={(e) => setInput(e.target.value)}
              onKeyPress={handleKeyPress}
              placeholder={
                usePrimalLLM 
                  ? `üß† Primal LLM (${llmConfig.num_layers}L/${llmConfig.num_heads}H): Secure mathematical AI - ask anything or try [1,2,3]...` 
                  : useGPT4All
                    ? `üîí GPT4All (${selectedModel}): Completely private AI processing - no data leaves your device...`
                    : useLMStudio
                      ? `üõ°Ô∏è LM Studio (${selectedModel}): Local AI server - 100% private and secure...`
                      : useOllama
                        ? `üè† Ollama (${selectedModel}): Local model inference - privacy guaranteed...`
                        : "üíæ Local LLM: Basic secure processing with mathematical principles..."
              }
              className="w-full bg-black/40 border border-indigo-500/30 rounded-lg px-4 py-3 text-white placeholder-gray-400 focus:outline-none focus:ring-2 focus:ring-indigo-500 focus:border-transparent resize-none"
              rows="3"
            />
            <div className="absolute bottom-2 right-2 text-xs text-gray-500 flex items-center space-x-2">
              <span>{usePrimalLLM ? 'üß†' : useGPT4All ? 'üîí' : useLMStudio ? 'üõ°Ô∏è' : useOllama ? 'üè†' : 'üíæ'}</span>
              <span>T:{llmConfig.temperature}</span>
              <span>p:{llmConfig.top_p}</span>
              <span>k:{llmConfig.top_k}</span>
              <span className="text-green-400">üîê PRIVATE</span>
            </div>500 flex items-center space-x-2">
              <span>{usePrimalLLM ? 'üß†' : useCloudLLM ? '‚ö°' : 'üíæ'}</span>
              <span>T:{llmConfig.temperature}</span>
              <span>p:{llmConfig.top_p}</span>
              <span>k:{llmConfig.top_k}</span>
            </div>
          </div>
          <button
            onClick={handleSend}
            disabled={isLoading || !input.trim()}
            className={`px-6 py-3 rounded-lg font-medium transition-all flex items-center space-x-2 ${
              usePrimalLLM 
                ? 'bg-indigo-600 hover:bg-indigo-700 text-white' 
                : useGPT4All
                  ? 'bg-green-600 hover:bg-green-700 text-white'
                  : useLMStudio
                    ? 'bg-purple-600 hover:bg-purple-700 text-white'
                    : useOllama
                      ? 'bg-orange-600 hover:bg-orange-700 text-white'
                      : 'bg-blue-600 hover:bg-blue-700 text-white'
            } disabled:opacity-50 disabled:cursor-not-allowed`}
          >
            <Send className="w-4 h-4" />
            <span>Generate</span>
          </button>
        </div>
        {/* Real-time LLM status */}
        <div className="flex items-center justify-between mt-3 text-xs text-gray-400">
          <div className="flex space-x-4">
            <span>üîß Temp: {llmConfig.temperature} | Top-p: {llmConfig.top_p} | Top-k: {llmConfig.top_k}</span>
            <span>üèóÔ∏è {llmConfig.num_layers}L/{llmConfig.num_heads}H | {(llmConfig.vocab_size/1000).toFixed(0)}K vocab</span>
            <span className="text-yellow-400">üìê Œ±: {llmConfig.alpha.toFixed(4)} | Œª: {llmConfig.lambda.toFixed(4)}</span>
          </div>
          <div className="text-green-500 flex items-center space-x-2">
            <span className="w-2 h-2 bg-green-400 rounded-full animate-pulse"></span>
            <span>üîí Private Primal LLM v1.0 | 100% Secure</span>
          </div>
        </div>
      </div>
    </div>
  );
};

export default PrimalLLM;
