
  const [messages, setMessages] = useState([]);
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [useCloudLLM, setUseCloudLLM] = useState(false);
  const [usePrimalLLM, setUsePrimalLLM] = useState(true);
  const [useOpenAI, setUseOpenAI] = useState(false);
  const [useGemini, setUseGemini] = useState(false);
  const [useOllama, setUseOllama] = useState(false);
  const [useHuggingFace, setUseHuggingFace] = useState(false);
  const [useGPT4All, setUseGPT4All] = useState(false);
  const [useLMStudio, setUseLMStudio] = useState(false);
  const [apiError, setApiError] = useState('');
  const [selectedModel, setSelectedModel] = useState('llama3.1');
    localOnly: true,
    noCloudAPI: true,
    encryptedStorage: true,
    noTelemetry: true
  const [primalLLM, setPrimalLLM] = useState(null);
    tokens_processed: 0,
    sequences_generated: 0,
    convergence_score: 0.0,
    model_temperature: 0.7,
    context_length: 2048
  
    // Production mathematical parameters
    dt: 1,
    half_life: 6,
    lambda: 0.115525,    // ln(2)/H
    alpha: 0.109101,     // 1 - exp(-lambda*dt)
    
    // TDT (Temporal Displacement Theory) parameters
    tdt_enabled: true,
    message_aging_factor: 0.95,  // How fast messages lose relevance
    confidence_decay: 0.88,      // Confidence degradation over time
    swarm_coherence: 0.92,       // Inter-node synchronization factor
    causality_weight: 1.47,      // Temporal causality importance
    
    // LLM-specific parameters
    vocab_size: 50000,
    embedding_dim: 768,
    num_layers: 12,
    num_heads: 12,
    max_seq_length: 2048,
    temperature: 0.7,
    top_p: 0.9,
    top_k: 50

  const messagesEndRef = useRef(null);


    scrollToBottom();

  // Initialize Production Primal Large Language Model
          dt: 1,
          half_life: 6,
          lambda: 0.115525,
          alpha: 0.109101
        // Add your initialization code here
        console.error("Error:", error);
    initializePrimalLLM();
        // Your code here
        console.error("Error:", error);
          // Core mathematical parameters
          dt: llmConfig.dt,
          half_life: llmConfig.half_life,
          lambda: llmConfig.lambda,
          alpha: llmConfig.alpha,
          
          // LLM architecture parameters
          vocab_size: llmConfig.vocab_size,
          embedding_dim: llmConfig.embedding_dim,
          num_layers: llmConfig.num_layers,
          num_heads: llmConfig.num_heads,
          max_seq_length: llmConfig.max_seq_length,
          temperature: llmConfig.temperature,
          top_p: llmConfig.top_p,
          top_k: llmConfig.top_k

        // Deterministic RNG for reproducible generation
        const alphaFromLambda = (lambda, dt=1) => 1 - Math.exp(-lambda * dt);
        const lambdaFromAlpha = (alpha, dt=1) => -Math.log(1 - alpha) / dt;
          const maxLogit = Math.max(...logits);
          const exps = logits.map(x => Math.exp((x - maxLogit) / temperature));
          const sumExps = exps.reduce((a, b) => a + b, 0);
          return exps.map(x => x / sumExps);

        // TDT (Temporal Displacement Theory) integration
          // ATAC message timestamp analysis
            const messageAge = (currentTime - timestamp) / 1000; // seconds
            const ageWeight = Math.exp(-cfg.message_aging_factor * messageAge);
            const causalityFactor = Math.pow(ageWeight, cfg.causality_weight);
          
          // Confidence scoring with temporal decay
            const temporalDecay = Math.exp(-cfg.confidence_decay * messageAge);
            const tdtConfidence = baseConfidence * temporalDecay;
            return Math.max(0.1, Math.min(1.0, tdtConfidence)); // Bounded confidence
          
          // Swarm collision avoidance through message weighting
            let totalCoherence = 0;
            let validNodes = 0;
            
                const distanceFactor = 1.0 / (1.0 + Math.abs(node.position - currentNodeId));
                const timingFactor = Math.exp(-cfg.lambda * node.latency);
                const nodeCoherence = distanceFactor * timingFactor * cfg.swarm_coherence;
                totalCoherence += nodeCoherence;
                validNodes++;
            
            return validNodes > 0 ? totalCoherence / validNodes : cfg.swarm_coherence;
          
          // Latency handling with causality awareness
            const latencySeconds = latencyMs / 1000.0;
            const latencyPenalty = Math.exp(-cfg.alpha * latencySeconds);
            const priorityBoost = Math.pow(priority, cfg.causality_weight);
            return latencyPenalty * priorityBoost;
        // Mathematical helpers for LLM operations
          let s = seed >>> 0;

        // Token embedding using mathematical primal logic
          const embedding = new Array(cfg.embedding_dim);
            const pos_encoding = Math.sin(position / Math.pow(10000, 2 * i / cfg.embedding_dim));
            const primal_component = cfg.alpha * Math.exp(-cfg.lambda * (i / cfg.embedding_dim));
            const harmonic = Math.sin((token + i) * cfg.lambda) * cfg.alpha;
            embedding[i] = pos_encoding + primal_component + harmonic;
          return embedding;

        // Attention mechanism with primal mathematical weighting
          const d_k = query.length;
          const scores = [];
          
            let score = 0;
              score += query[j] * key[i][j];
            // Apply primal mathematical weighting
            const primal_weight = Math.exp(-cfg.lambda * i) * cfg.alpha;
            score = score / Math.sqrt(d_k) * (1 + primal_weight);
            scores.push(score);
          
          const attention_weights = softmax(scores, cfg.temperature);
          const output = new Array(value[0].length).fill(0);
          
              output[j] += attention_weights[i] * value[i][j];
          

          cfg,
          rng: createRNG(0xABCDEF),
            // Simple character-level tokenizer for demo
              return text.split('').map(char => char.charCodeAt(0) % cfg.vocab_size);
              return tokens.map(token => String.fromCharCode((token % 95) + 32)).join('');
          
          // Core LLM text generation
              // Text generation mode
              const tokens = llmSystem.tokenizer.encode(prompt);
              const generated_tokens = [];
              
                const context = [...tokens, ...generated_tokens].slice(-cfg.max_seq_length);
                const next_token = llmSystem.predictNextToken(context);
                generated_tokens.push(next_token);
                
                // Stop on period or exclamation for natural sentences
                if (next_token === 46 || next_token === 33) break;
              
              return llmSystem.tokenizer.decode([...tokens, ...generated_tokens]);
              // Sequence generation mode (backward compatibility)
              return llmSystem.generateSequence(prompt, max_tokens);

          // Enhanced sequence generation with LLM principles
            const seq = prompt.slice();
            if (seq.length === 0) seq.push(0);
            
              const context_length = Math.min(seq.length, 10);
              const context = seq.slice(-context_length);
              
              // Apply transformer-like attention to sequence elements
              const embeddings = context.map((val, pos) => tokenEmbedding(val, pos));
              const attention_result = primalAttention(
                embeddings[embeddings.length - 1], 
                embeddings, 
                embeddings
              );
              
              // Generate next value using attention-weighted context
              const weighted_sum = attention_result.output.reduce((a, b) => a + b, 0);
              const primal_component = seq[seq.length - 1] * (1 + cfg.alpha * Math.exp(-cfg.lambda * (i + 1)));
              const harmonic_component = (seq[seq.length - 2] || 0) * (cfg.alpha * Math.sin((i + 1) * cfg.lambda));
              
              const next = Math.round(primal_component + harmonic_component + weighted_sum * 0.1);
              seq.push(next);
            return seq;

          // Predict next token using primal mathematical principles
            const context_embeddings = context.map((token, pos) => tokenEmbedding(token, pos));
            
            // Multi-head attention simulation
            const num_heads = cfg.num_heads;
            const head_outputs = [];
            
              const query = context_embeddings[context_embeddings.length - 1];
              const attention_result = primalAttention(query, context_embeddings, context_embeddings);
              head_outputs.push(attention_result.output);
            
            // Combine heads and apply feedforward with primal logic
              return head_outputs.reduce((sum, head) => sum + head[i], 0) / num_heads;
            
            // Generate logits for vocabulary
            const logits = [];
              let logit = 0;
                logit += combined_output[i] * Math.sin((token + i) * cfg.lambda);
              logits.push(logit);
            
            // Apply temperature and sample
            const probabilities = softmax(logits, cfg.temperature);
            
            // Top-k and top-p sampling
              .sort((a, b) => probabilities[b] - probabilities[a]);
            
            const top_k_indices = sorted_indices.slice(0, cfg.top_k);
            let cumulative_prob = 0;
            const top_p_indices = [];
            
              cumulative_prob += probabilities[idx];
              top_p_indices.push(idx);
              if (cumulative_prob >= cfg.top_p) break;
            
            // Sample from filtered distribution
            const random_val = llmSystem.rng();
            let cumulative = 0;
              cumulative += probabilities[idx];
              if (random_val <= cumulative) return idx;
            
            return top_p_indices[0] || 32; // fallback to space character

          // Analyze text patterns using LLM insights
            const tokens = llmSystem.tokenizer.encode(text);
            
              token_frequencies[token] = (token_frequencies[token] || 0) + 1;
            
              bigrams[bigram] = (bigrams[bigram] || 0) + 1;
            
            const vocab_diversity = Object.keys(token_frequencies).length;
            const avg_token_freq = tokens.length / vocab_diversity;
            const entropy = Object.values(token_frequencies)
              .map(freq => freq / tokens.length)
              .reduce((sum, p) => sum - p * Math.log2(p), 0);
            
              text_length: text.length,
              token_count: tokens.length,
              vocab_diversity,
              avg_token_frequency: avg_token_freq,
              entropy,
              estimated_perplexity: Math.pow(2, entropy),
              mathematical_complexity: entropy * cfg.alpha + vocab_diversity * cfg.lambda

          // Get LLM model parameters and statistics
            vocab_size: cfg.vocab_size,
            embedding_dim: cfg.embedding_dim,
            num_layers: cfg.num_layers,
            num_heads: cfg.num_heads,
            max_seq_length: cfg.max_seq_length,
            temperature: cfg.temperature,
            top_p: cfg.top_p,
            top_k: cfg.top_k,
            alpha: cfg.alpha,
            lambda: cfg.lambda,
            half_life: cfg.half_life,
            total_parameters: cfg.vocab_size * cfg.embedding_dim + cfg.num_layers * cfg.num_heads * cfg.embedding_dim * cfg.embedding_dim

          // Update model configuration
            Object.assign(cfg, newConfig);
              cfg.lambda = Math.log(2) / newConfig.half_life;
              cfg.alpha = alphaFromLambda(cfg.lambda, cfg.dt);

        setPrimalLLM(llmSystem);
        
        // Update model statistics
          tokens_processed: 0,
          sequences_generated: 0,
          convergence_score: llmSystem.cfg.alpha,
          model_temperature: llmSystem.cfg.temperature,
          context_length: llmSystem.cfg.max_seq_length
        
        console.error("Failed to initialize Primal LLM:", error);
    
    initializePrimalLLM();

  // Process text using Primal Large Language Model
      throw new Error("Primal LLM not initialized");

      // Check if input contains sequences (backward compatibility)
      const numberRegex = /\[([0-9,\s]+)\]/g;
      const sequences = [];
      let match;
      
        const nums = match[1].split(',').map(n => parseInt(n.trim())).filter(n => !isNaN(n));
        if (nums.length > 0) sequences.push(nums);
      
      let response = "üß† **Primal Large Language Model Response:**\n\n";
      
        // Sequence generation mode
        response += "**Mathematical Sequence Generation:**\n";
          const result = primalLLM.generateSequence(prompt, 8);
          const generated = result.slice(prompt.length);
          
        
        const stats = primalLLM.getModelStats();
        response += `\n**Model Architecture:**\n`;
        
        // Text generation mode
        response += "**Natural Language Generation:**\n";
        
        // Analyze input text
        const analysis = primalLLM.analyzeText(userMessage);
        response += `**Input Analysis:**\n`;
        
        // Generate response using LLM
        const llm_response = primalLLM.generate(userMessage, 50);
        response += `**LLM Generated Response:**\n`;
        
        // Provide contextual assistance
        response += `**Contextual Assistance:**\n`;
          response += `I'm a Primal Large Language Model with mathematical convergence principles. I can help with:\n`;
          response += `‚Ä¢ **Creative writing** with mathematical harmony\n`;
          response += `‚Ä¢ **Technical analysis** using attention mechanisms\n`;
          response += `‚Ä¢ **Sequence prediction** with proven stability\n`;
          response += `‚Ä¢ **Pattern recognition** in text and numbers\n`;
          response += `I can assist with programming tasks using my mathematical foundation:\n`;
          response += `‚Ä¢ Algorithm design with convergence guarantees\n`;
          response += `‚Ä¢ Code optimization using attention principles\n`;
          response += `‚Ä¢ Mathematical modeling and simulation\n`;
            response += `‚Ä¢ Breaking down complex topics into simpler components\n`;
            response += `‚Ä¢ Using mathematical frameworks for analysis\n`;
            response += `‚Ä¢ Expanding on the topic with more detailed examples\n`;
            response += `‚Ä¢ Exploring related mathematical concepts\n`;
        
        // Update statistics
          ...prev,
          tokens_processed: prev.tokens_processed + analysis.token_count,
          sequences_generated: prev.sequences_generated + 1
      
      response += `\n**Model Status:**\n`;
      
      return response;
      
      console.error("Primal LLM Error:", error);

  // GPT4All local model integration (completely offline)
      // GPT4All runs completely locally with no internet connection
        method: "POST",
          model: selectedModel,
          messages: [
              role: "system", 
          ],
          temperature: llmConfig.temperature,
          max_tokens: 500,
          stream: false


      const data = await response.json();
      setApiError('');
      return data.choices[0].message.content;
      console.error("GPT4All Error:", error);
      throw error;

  // LM Studio local model integration (completely offline)
      // LM Studio runs completely locally
        method: "POST",
          "Content-Type": "application/json",
          "Authorization": "Bearer lm-studio"
          model: selectedModel,
          messages: [
              role: "system",
          ],
          temperature: llmConfig.temperature,
          max_tokens: 1000,
          stream: false


      const data = await response.json();
      setApiError('');
      return data.choices[0].message.content;
      console.error("LM Studio Error:", error);
      throw error;

  // Enhanced Ollama with privacy guarantees
      // Ollama runs completely locally with no telemetry
        method: "POST",
          model: selectedModel,
          stream: false,
            temperature: llmConfig.temperature,
            top_p: llmConfig.top_p,
            top_k: llmConfig.top_k,
            num_predict: 500,
            stop: ["User:", "\n\nUser:"]


      const data = await response.json();
      setApiError('');
      console.error("Ollama Error:", error);
      throw error;

  // Enhanced local processing with complete privacy
    // All processing happens locally with no external connections
      text_length: userMessage.length,
      privacy_status: "FULLY_LOCAL"
    
    const localResponses = [
      `üõ°Ô∏è **SECURE LOCAL SESSION** - All processing happens on your device. Zero external data transmission.`,
      `üîê **PRIVACY GUARANTEED** - Your conversation stays completely private using local mathematical LLM.`,
    ];
    
    const response = localResponses[Math.floor(Math.random() * localResponses.length)];

  // Disable all cloud APIs for privacy
    throw new Error("OpenAI disabled for privacy - use local models only"); 
    throw new Error("Google Gemini disabled for privacy - use local models only"); 
    throw new Error("HuggingFace API disabled for privacy - use local models only"); 
    throw new Error("Claude API disabled for privacy - use local models only"); 
    const conversationHistory = [
        role: "system",
        content: `You are comparing responses with a Primal Large Language Model that uses mathematical convergence principles.

The Primal LLM Architecture:

Provide responses that complement the mathematical approach of the Primal LLM while showcasing your own capabilities. Be helpful, insightful, and ready to collaborate on any topic.`
    ];

    // Add recent conversation context
    const recentMessages = messages.slice(-5);
        role: msg.role === 'user' ? 'user' : 'assistant',
        content: msg.content

      role: "user",
      content: userMessage

        method: "POST",
          "Content-Type": "application/json",
          model: "claude-sonnet-4-20250514",
          max_tokens: 2000,
          messages: conversationHistory.slice(1)


      const data = await response.json();
      setApiError('');
      return data.content[0].text;
      console.error("Claude API Error:", error);
      setApiError(error.message);
      throw error;

  // Local processing with basic LLM simulation
    
    const responses = [
      `Using local language model with mathematical convergence principles. How can I help optimize your workflow?`,
    ];
    
    return responses[Math.floor(Math.random() * responses.length)] + "\n\n*Note: For full capabilities, try Primal LLM mode for mathematical language processing.*";

    if (!input.trim()) return;

      role: 'user',
      content: input,
      timestamp: Date.now()

    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setIsLoading(true);
    setApiError('');

      let response;
      
        response = await processPrimalLLM(input);
        response = await sendToGPT4All(input);
        response = await sendToLMStudio(input);
        response = await sendToOllama(input);
        response = await processLocalLLM(input);

        role: 'assistant',
        content: response,
        timestamp: Date.now()

      setMessages(prev => [...prev, assistantMessage]);
        role: 'assistant',
        timestamp: Date.now()
      setMessages(prev => [...prev, errorMessage]);
      
        const fallbackResponse = await processLocalLLM(input);
          role: 'assistant',
          content: fallbackResponse,
          timestamp: Date.now()
        setMessages(prev => [...prev, fallbackMessage]);
        console.error("Fallback failed:", fallbackError);

    setIsLoading(false);

      e.preventDefault();
      handleSend();

      
        newConfig.temperature = Math.max(0.1, Math.min(2.0, prev.temperature + delta));
        newConfig.top_p = Math.max(0.1, Math.min(1.0, prev.top_p + delta));
        newConfig.top_k = Math.max(1, Math.min(100, prev.top_k + delta));
        newConfig.num_layers = Math.max(1, Math.min(24, prev.num_layers + delta));
        newConfig.num_heads = Math.max(1, Math.min(16, prev.num_heads + delta));
        newConfig.half_life = Math.max(1, Math.min(20, prev.half_life + delta));
        newConfig.lambda = Math.log(2) / newConfig.half_life;
        newConfig.alpha = 1 - Math.exp(-newConfig.lambda * newConfig.dt);
      
      return newConfig;

    setMessages([]);
    setApiError('');

  return (
    <div className="flex flex-col h-screen bg-gradient-to-br from-slate-900 via-indigo-900 to-slate-900">
      <div className="bg-black/20 backdrop-blur-sm border-b border-indigo-500/30 p-4">
        <div className="flex items-center justify-between">
          <div className="flex items-center space-x-3">
            <div className="relative">
              <Brain className="w-8 h-8 text-indigo-400 animate-pulse" />
              <div className="absolute -top-1 -right-1 w-3 h-3 bg-green-400 rounded-full animate-bounce"></div>
            </div>
            <div>
              <h1 className="text-xl font-bold text-white">üîí Private Primal Large Language Model</h1>
            </div>
          </div>
          
          <div className="flex items-center space-x-4">
            <div className="text-xs text-indigo-300 text-right">
              <div>üîí Privacy: 100% Local</div>
            </div>
            <button
              className="px-3 py-1 bg-red-600/20 text-red-300 rounded text-sm hover:bg-red-600/30"
            >
              Clear
            </button>
            <div className="flex items-center space-x-2">
              <span className="text-sm text-gray-300">üîí Private LLM:</span>
              <button
                  setUsePrimalLLM(true);
                  setUseGPT4All(false);
                  setUseLMStudio(false);
                  setUseOllama(false);
                  usePrimalLLM ? 'bg-indigo-500 text-white' : 'bg-gray-600 text-gray-300'
              >
                <Brain className="w-3 h-3" />
                <span>Primal</span>
              </button>
              <button
                  setUsePrimalLLM(false);
                  setUseGPT4All(true);
                  setUseLMStudio(false);
                  setUseOllama(false);
                  useGPT4All ? 'bg-green-500 text-white' : 'bg-gray-600 text-gray-300'
              >
                <Cpu className="w-3 h-3" />
                <span>GPT4All</span>
              </button>
              <button
                  setUsePrimalLLM(false);
                  setUseGPT4All(false);
                  setUseLMStudio(true);
                  setUseOllama(false);
                  useLMStudio ? 'bg-purple-500 text-white' : 'bg-gray-600 text-gray-300'
              >
                <Settings className="w-3 h-3" />
                <span>LM Studio</span>
              </button>
              <button
                  setUsePrimalLLM(false);
                  setUseGPT4All(false);
                  setUseLMStudio(false);
                  setUseOllama(true);
                  useOllama ? 'bg-orange-500 text-white' : 'bg-gray-600 text-gray-300'
              >
                <Download className="w-3 h-3" />
                <span>Ollama</span>
              </button>
            </div>
          </div>
        </div>
      </div>

        <div className="bg-orange-900/20 p-2 border-b border-orange-500/20">
          <div className="flex items-center space-x-4 text-xs text-orange-300">
            <span>Ollama Model:</span>
            <select 
              className="bg-black/40 border border-orange-500/30 rounded px-2 py-1 text-white text-xs"
            >
              <option value="llama3.1">Llama 3.1 (8B)</option>
              <option value="llama3.1:70b">Llama 3.1 (70B)</option>
              <option value="mistral">Mistral (7B)</option>
              <option value="codellama">CodeLlama (7B)</option>
              <option value="phi3">Phi-3 (3.8B)</option>
              <option value="gemma2">Gemma 2 (9B)</option>
              <option value="qwen2">Qwen 2 (7B)</option>
            </select>
            <span className="text-gray-400">‚Ä¢</span>
          </div>
        </div>

      <div className="bg-black/10 backdrop-blur-sm p-3 border-b border-indigo-500/20">
        <div className="grid grid-cols-8 gap-2 text-xs">
          <div className="flex items-center space-x-1">
            <span className="text-indigo-300">Temp:</span>
          </div>
          <div className="flex items-center space-x-1">
            <span className="text-indigo-300">Top-p:</span>
          </div>
          <div className="flex items-center space-x-1">
            <span className="text-indigo-300">Top-k:</span>
          </div>
          <div className="flex items-center space-x-1">
            <span className="text-green-300">Layers:</span>
          </div>
          <div className="flex items-center space-x-1">
            <span className="text-green-300">Heads:</span>
          </div>
          <div className="flex items-center space-x-1">
            <span className="text-yellow-300">H-life:</span>
          </div>
          <div className="flex items-center space-x-1">
            <span className="text-purple-300">Œ±:</span>
          </div>
          <div className="flex items-center space-x-1">
            <span className="text-purple-300">Œª:</span>
          </div>
        </div>
      </div>

      <div className="bg-indigo-900/20 p-2 border-b border-indigo-500/20">
        <div className="flex items-center justify-between text-xs text-indigo-300">
          <div className="flex space-x-4">
          </div>
          <div className="flex space-x-4">
              usePrimalLLM ? 'text-indigo-400' :
              useGPT4All ? 'text-green-400' :
              useLMStudio ? 'text-purple-400' :
              useOllama ? 'text-orange-400' : 'text-blue-400'
                usePrimalLLM ? 'Primal LLM (Private)' :
            </span>
            <span className="text-green-400">üîí 100% Private</span>
          </div>
        </div>
      </div>

        <div className="bg-red-900/30 border-l-4 border-red-500 p-3 flex items-center space-x-2">
          <AlertCircle className="w-5 h-5 text-red-400" />
        </div>

      <div className="flex-1 overflow-y-auto p-4 space-y-4">
          <div className="text-center py-8">
            <div className="flex justify-center space-x-4 mb-4">
              <Brain className="w-16 h-16 text-indigo-400 animate-pulse" />
              <Network className="w-16 h-16 text-green-400 animate-bounce" />
              <Cpu className="w-16 h-16 text-purple-400 animate-pulse" />
            </div>
            <h3 className="text-2xl font-bold text-white mb-3">üîí Private Primal Large Language Model</h3>
            <p className="text-gray-300 mb-6">Secure mathematical transformer with zero data transmission</p>
            
            <div className="grid grid-cols-1 md:grid-cols-4 gap-4 text-sm mb-8">
              <div className="bg-indigo-900/30 p-4 rounded-lg border border-indigo-500/30">
                <h4 className="text-indigo-400 font-semibold mb-3 flex items-center">
                  <Brain className="w-5 h-5 mr-2" />
                  Primal LLM
                </h4>
                <div className="text-gray-300 space-y-1 text-xs">
                  <p>‚Ä¢ Mathematical transformer</p>
                  <p>‚Ä¢ Convergence principles</p>
                  <p>‚Ä¢ 100% local processing</p>
                </div>
              </div>
              
              <div className="bg-green-900/30 p-4 rounded-lg border border-green-500/30">
                <h4 className="text-green-400 font-semibold mb-3 flex items-center">
                  <Cpu className="w-5 h-5 mr-2" />
                  GPT4All
                </h4>
                <div className="text-gray-300 space-y-1 text-xs">
                  <p>‚Ä¢ Complete offline operation</p>
                  <p>‚Ä¢ Multiple model options</p>
                  <p>‚Ä¢ Zero internet dependency</p>
                  <p>‚Ä¢ Privacy guaranteed</p>
                </div>
              </div>
              
              <div className="bg-purple-900/30 p-4 rounded-lg border border-purple-500/30">
                <h4 className="text-purple-400 font-semibold mb-3 flex items-center">
                  <Settings className="w-5 h-5 mr-2" />
                  LM Studio
                </h4>
                <div className="text-gray-300 space-y-1 text-xs">
                  <p>‚Ä¢ Local model server</p>
                  <p>‚Ä¢ Easy model management</p>
                  <p>‚Ä¢ Desktop application</p>
                  <p>‚Ä¢ Secure local inference</p>
                </div>
              </div>
              
              <div className="bg-orange-900/30 p-4 rounded-lg border border-orange-500/30">
                <h4 className="text-orange-400 font-semibold mb-3 flex items-center">
                  <Download className="w-5 h-5 mr-2" />
                  Ollama
                </h4>
                <div className="text-gray-300 space-y-1 text-xs">
                  <p>‚Ä¢ Command-line interface</p>
                  <p>‚Ä¢ Llama, Mistral, CodeLlama</p>
                  <p>‚Ä¢ Local server deployment</p>
                  <p>‚Ä¢ No data transmission</p>
                </div>
              </div>
            </div>

            <div className="bg-black/40 p-6 rounded-lg mb-6 border border-green-500/30">
              <h4 className="text-green-400 font-semibold mb-3 flex items-center justify-center">
                <span className="mr-2">üîí</span>
                Privacy & Security Guarantees
              </h4>
              <div className="grid grid-cols-2 md:grid-cols-4 gap-4 text-xs text-gray-300">
                <div className="text-center">
                  <div className="text-lg font-bold text-green-400">100%</div>
                  <div>Local Processing</div>
                </div>
                <div className="text-center">
                  <div className="text-lg font-bold text-green-400">0</div>
                  <div>Data Transmission</div>
                </div>
                <div className="text-center">
                  <div className="text-lg font-bold text-green-400">‚àû</div>
                  <div>Privacy Protection</div>
                </div>
                <div className="text-center">
                  <div className="text-lg font-bold text-green-400">üõ°Ô∏è</div>
                  <div>Secure by Design</div>
                </div>
              </div>
              <div className="mt-4 text-center text-sm text-green-300">
                <p>üîê All processing happens on your device ‚Ä¢ üö´ No cloud APIs ‚Ä¢ üõ°Ô∏è Zero telemetry ‚Ä¢ üîí Encrypted local storage</p>
              </div>
            </div>
            
            <div className="space-y-3">
              <p className="text-green-300 text-sm">üîí <strong>100% Private:</strong> All models run locally - GPT4All, LM Studio, Ollama, Primal LLM</p>
              <p className="text-blue-300 text-sm">üßÆ <strong>Mathematical Analysis:</strong> [1, 2, 3] or text for secure sequence/pattern analysis</p>
              <p className="text-purple-300 text-sm">üèóÔ∏è <strong>Local Architecture:</strong> Full transformer models with mathematical convergence</p>
              <p className="text-yellow-300 text-sm">üõ°Ô∏è <strong>Security First:</strong> Zero data transmission, encrypted processing, no telemetry</p>
            </div>
          </div>
        
              message.role === 'user' 
                ? 'bg-indigo-600 text-white' 
                : 'bg-black/40 text-gray-100 border border-indigo-500/30'
              <div className="flex items-center space-x-2 mb-2">
                  <User className="w-4 h-4" />
                ) : (
                  <div className="flex items-center space-x-1">
                    <Brain className="w-4 h-4 text-indigo-400" />
                  </div>
                <span className="text-xs opacity-70">
                </span>
                  <span className="text-xs opacity-50 flex items-center space-x-2">
                  </span>
              </div>
            </div>
          </div>
        
          <div className="flex justify-start">
            <div className="bg-black/40 border border-indigo-500/30 rounded-lg p-4">
              <div className="flex items-center space-x-3">
                <Brain className="w-5 h-5 text-indigo-400 animate-pulse" />
                <div className="flex space-x-1">
                  <div className="w-2 h-2 bg-indigo-400 rounded-full animate-bounce"></div>
                </div>
                <span className="text-sm text-gray-400">
                </span>
              </div>
            </div>
          </div>
        
      </div>

      <div className="p-4 bg-black/20 backdrop-blur-sm border-t border-indigo-500/30">
        <div className="flex space-x-3">
          <div className="flex-1 relative">
            <textarea
                usePrimalLLM 
                  : useGPT4All
                    : useLMStudio
                      : useOllama
                        : "üíæ Local LLM: Basic secure processing with mathematical principles..."
              className="w-full bg-black/40 border border-indigo-500/30 rounded-lg px-4 py-3 text-white placeholder-gray-400 focus:outline-none focus:ring-2 focus:ring-indigo-500 focus:border-transparent resize-none"
              rows="3"
            />
            <div className="absolute bottom-2 right-2 text-xs text-gray-500 flex items-center space-x-2">
              <span className="text-green-400">üîê PRIVATE</span>
            </div>500 flex items-center space-x-2">
            </div>
          </div>
          <button
              usePrimalLLM 
                ? 'bg-indigo-600 hover:bg-indigo-700 text-white' 
                : useGPT4All
                  ? 'bg-green-600 hover:bg-green-700 text-white'
                  : useLMStudio
                    ? 'bg-purple-600 hover:bg-purple-700 text-white'
                    : useOllama
                      ? 'bg-orange-600 hover:bg-orange-700 text-white'
                      : 'bg-blue-600 hover:bg-blue-700 text-white'
          >
            <Send className="w-4 h-4" />
            <span>Generate</span>
          </button>
        </div>
        <div className="flex items-center justify-between mt-3 text-xs text-gray-400">
          <div className="flex space-x-4">
          </div>
          <div className="text-green-500 flex items-center space-x-2">
            <span className="w-2 h-2 bg-green-400 rounded-full animate-pulse"></span>
            <span>üîí Private Primal LLM v1.0 | 100% Secure</span>
          </div>
        </div>
      </div>
    </div>
  );

export default PrimalLLM;
