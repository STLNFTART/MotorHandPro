{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/STLNFTART/MotorHandPro/blob/main/notebooks/01_primal_logic_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primal Logic Framework: Introduction\n",
    "\n",
    "This notebook provides an interactive introduction to the **Primal Logic control framework** used in MotorHandPro for robotic actuator control, biomedical simulation, space mission planning, and autonomous systems.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Lightfoot Constant (λ)**: λ = 0.16905 s⁻¹ - Exponential memory weighting factor\n",
    "- **Donte Constant (D)**: D = 149.9992314000 - Fixed point for bounded convergence\n",
    "- **Exponential Memory Weighting**: Ensures bounded stability without integral windup\n",
    "- **Lipschitz Continuity**: Mathematically proven convergence guarantees\n",
    "\n",
    "## Installation\n",
    "\n",
    "First, let's install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install dependencies for Google Colab\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install numpy matplotlib scipy pandas\n",
    "    # Clone the repository if in Colab\n",
    "    !git clone https://github.com/STLNFTART/MotorHandPro.git\n",
    "    sys.path.append('/content/MotorHandPro')\n",
    "else:\n",
    "    # Local environment\n",
    "    sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import pandas as pd"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Primal Logic Constants\n",
    "\n",
    "The Primal Logic framework is built on two fundamental constants that ensure bounded convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Fundamental constants\n",
    "LAMBDA = 0.16905  # Lightfoot constant (s^-1)\n",
    "DONTE = 149.9992314000  # Donte constant (fixed point)\n",
    "GAMMA_PRECISION = 1e-8  # Convergence precision\n",
    "\n",
    "print(f\"Lightfoot Constant (λ): {LAMBDA} s⁻¹\")\n",
    "print(f\"Donte Constant (D): {DONTE}\")\n",
    "print(f\"Convergence Precision: {GAMMA_PRECISION}\")\n",
    "\n",
    "# Physical interpretation\n",
    "print(f\"\\nMemory half-life: {np.log(2)/LAMBDA:.2f} seconds\")\n",
    "print(f\"Time constant (τ): {1/LAMBDA:.2f} seconds\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exponential Memory Weighting\n",
    "\n",
    "The core innovation of Primal Logic is the exponential memory weighting function:\n",
    "\n",
    "$$w(t) = e^{-\\lambda t}$$\n",
    "\n",
    "This provides bounded memory without integral windup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize exponential memory weighting\n",
    "t = np.linspace(0, 50, 1000)\n",
    "w = np.exp(-LAMBDA * t)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(t, w, 'b-', linewidth=2)\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='50% weight')\n",
    "plt.axvline(x=np.log(2)/LAMBDA, color='r', linestyle='--', alpha=0.5, label=f'Half-life: {np.log(2)/LAMBDA:.1f}s')\n",
    "plt.xlabel('Time (s)', fontsize=12)\n",
    "plt.ylabel('Weight w(t)', fontsize=12)\n",
    "plt.title('Exponential Memory Weighting', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogy(t, w, 'b-', linewidth=2)\n",
    "plt.xlabel('Time (s)', fontsize=12)\n",
    "plt.ylabel('Weight w(t) (log scale)', fontsize=12)\n",
    "plt.title('Exponential Memory Weighting (Log Scale)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"After 1 time constant ({1/LAMBDA:.2f}s): {np.exp(-1):.1%} of original weight\")\n",
    "print(f\"After 3 time constants ({3/LAMBDA:.2f}s): {np.exp(-3):.1%} of original weight\")\n",
    "print(f\"After 5 time constants ({5/LAMBDA:.2f}s): {np.exp(-5):.1%} of original weight\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Primal Logic Control Law\n",
    "\n",
    "The Primal Logic control law combines proportional error with exponentially weighted history:\n",
    "\n",
    "$$u(t) = K_P \\cdot e(t) + K_E \\cdot \\int_0^t e^{-\\lambda(t-\\tau)} e(\\tau) d\\tau$$\n",
    "\n",
    "Where:\n",
    "- $K_P$ = Proportional gain\n",
    "- $K_E$ = Exponential memory gain\n",
    "- $e(t)$ = Error signal\n",
    "- $\\lambda$ = Lightfoot constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class PrimalLogicController:\n",
    "    \"\"\"Simple Primal Logic controller implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, KP=1.0, KE=0.5, lambda_val=LAMBDA):\n",
    "        self.KP = KP\n",
    "        self.KE = KE\n",
    "        self.lambda_val = lambda_val\n",
    "        self.memory = 0.0\n",
    "        self.last_time = 0.0\n",
    "        \n",
    "    def update(self, error, current_time, dt):\n",
    "        \"\"\"Update controller and return control signal\"\"\"\n",
    "        # Decay existing memory\n",
    "        decay = np.exp(-self.lambda_val * dt)\n",
    "        self.memory = self.memory * decay + error * dt\n",
    "        \n",
    "        # Compute control signal\n",
    "        u = self.KP * error + self.KE * self.memory\n",
    "        \n",
    "        self.last_time = current_time\n",
    "        return u\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset controller state\"\"\"\n",
    "        self.memory = 0.0\n",
    "        self.last_time = 0.0\n",
    "\n",
    "# Test the controller with a step response\n",
    "dt = 0.01  # 10ms timestep\n",
    "t_sim = np.arange(0, 10, dt)\n",
    "setpoint = 1.0\n",
    "state = 0.0\n",
    "states = []\n",
    "controls = []\n",
    "errors = []\n",
    "\n",
    "controller = PrimalLogicController(KP=2.0, KE=0.8)\n",
    "\n",
    "# Simple first-order plant: dx/dt = -x + u\n",
    "for t in t_sim:\n",
    "    error = setpoint - state\n",
    "    u = controller.update(error, t, dt)\n",
    "    \n",
    "    # Update plant (simple first-order system)\n",
    "    state += dt * (-state + u)\n",
    "    \n",
    "    states.append(state)\n",
    "    controls.append(u)\n",
    "    errors.append(error)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
    "\n",
    "axes[0].plot(t_sim, states, 'b-', linewidth=2, label='State')\n",
    "axes[0].axhline(y=setpoint, color='r', linestyle='--', alpha=0.5, label='Setpoint')\n",
    "axes[0].set_ylabel('State', fontsize=12)\n",
    "axes[0].set_title('Primal Logic Step Response', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(t_sim, controls, 'g-', linewidth=2)\n",
    "axes[1].set_ylabel('Control Signal u(t)', fontsize=12)\n",
    "axes[1].set_title('Control Signal', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(t_sim, errors, 'r-', linewidth=2)\n",
    "axes[2].set_ylabel('Error e(t)', fontsize=12)\n",
    "axes[2].set_xlabel('Time (s)', fontsize=12)\n",
    "axes[2].set_title('Tracking Error', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance metrics\n",
    "rise_time_idx = np.where(np.array(states) >= 0.9 * setpoint)[0][0]\n",
    "settling_error = np.abs(np.array(states[-100:]) - setpoint)\n",
    "overshoot = (np.max(states) - setpoint) / setpoint * 100\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"Rise time (to 90%): {t_sim[rise_time_idx]:.2f}s\")\n",
    "print(f\"Overshoot: {overshoot:.2f}%\")\n",
    "print(f\"Final steady-state error: {settling_error.mean():.6f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fixed-Point Convergence to Donte Constant\n",
    "\n",
    "The Donte constant emerges from the fixed-point iteration:\n",
    "\n",
    "$$x_{n+1} = f(x_n)$$\n",
    "\n",
    "where $f(x)$ is designed to converge to D = 149.9992314000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def fixed_point_iteration(x0, iterations=50):\n",
    "    \"\"\"Fixed-point iteration to demonstrate Donte constant convergence\"\"\"\n",
    "    # Example iteration function (simplified)\n",
    "    def f(x):\n",
    "        return 150.0 - 0.5 * np.sin(x / 10)\n",
    "    \n",
    "    x_values = [x0]\n",
    "    for i in range(iterations):\n",
    "        x_new = f(x_values[-1])\n",
    "        x_values.append(x_new)\n",
    "    \n",
    "    return np.array(x_values)\n",
    "\n",
    "# Test convergence from different initial conditions\n",
    "initial_conditions = [100, 120, 140, 160, 180, 200]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(initial_conditions)))\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for x0, color in zip(initial_conditions, colors):\n",
    "    x_vals = fixed_point_iteration(x0)\n",
    "    plt.plot(x_vals, linewidth=2, alpha=0.7, label=f'x₀={x0}', color=color)\n",
    "\n",
    "plt.axhline(y=DONTE, color='r', linestyle='--', linewidth=2, label=f'Donte constant: {DONTE}')\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Value', fontsize=12)\n",
    "plt.title('Convergence to Donte Constant', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for x0, color in zip(initial_conditions, colors):\n",
    "    x_vals = fixed_point_iteration(x0)\n",
    "    error = np.abs(x_vals - DONTE)\n",
    "    plt.semilogy(error, linewidth=2, alpha=0.7, label=f'x₀={x0}', color=color)\n",
    "\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Absolute Error (log scale)', fontsize=12)\n",
    "plt.title('Convergence Error', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lipschitz Continuity and Stability Analysis\n",
    "\n",
    "The Primal Logic framework guarantees convergence through Lipschitz continuity:\n",
    "\n",
    "$$|f(x) - f(y)| \\leq L|x - y|$$\n",
    "\n",
    "where $L < 1$ is the Lipschitz constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_lipschitz_constant(f, x_range, num_samples=1000):\n",
    "    \"\"\"Numerically estimate Lipschitz constant\"\"\"\n",
    "    x_vals = np.linspace(x_range[0], x_range[1], num_samples)\n",
    "    f_vals = f(x_vals)\n",
    "    \n",
    "    # Compute maximum slope\n",
    "    differences = np.abs(np.diff(f_vals))\n",
    "    dx = (x_range[1] - x_range[0]) / num_samples\n",
    "    slopes = differences / dx\n",
    "    \n",
    "    L = np.max(slopes)\n",
    "    return L, x_vals, f_vals\n",
    "\n",
    "# Example function with exponential decay\n",
    "def primal_function(x):\n",
    "    return x * np.exp(-LAMBDA * x / 10) + DONTE * (1 - np.exp(-LAMBDA * x / 10))\n",
    "\n",
    "L, x_vals, f_vals = compute_lipschitz_constant(primal_function, [0, 200])\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_vals, f_vals, 'b-', linewidth=2, label='f(x)')\n",
    "plt.plot(x_vals, x_vals, 'r--', alpha=0.5, label='y=x (fixed point line)')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('Primal Logic Function', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "derivatives = np.gradient(f_vals, x_vals)\n",
    "plt.plot(x_vals, np.abs(derivatives), 'g-', linewidth=2)\n",
    "plt.axhline(y=1.0, color='r', linestyle='--', linewidth=2, label='L=1 (stability boundary)')\n",
    "plt.axhline(y=L, color='orange', linestyle='--', linewidth=2, label=f'Estimated L={L:.4f}')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('|df/dx|', fontsize=12)\n",
    "plt.title('Lipschitz Constant Analysis', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLipschitz constant L: {L:.6f}\")\n",
    "if L < 1:\n",
    "    print(\"✓ System is stable (L < 1)\")\n",
    "else:\n",
    "    print(\"✗ System may be unstable (L >= 1)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Parameter Tuning\n",
    "\n",
    "Use the interactive widgets below to explore how different parameters affect system behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Try to import ipywidgets for interactive controls\n",
    "try:\n",
    "    from ipywidgets import interact, FloatSlider\n",
    "    \n",
    "    def interactive_control(KP=2.0, KE=0.8, lambda_val=LAMBDA):\n",
    "        \"\"\"Interactive controller parameter tuning\"\"\"\n",
    "        dt = 0.01\n",
    "        t_sim = np.arange(0, 10, dt)\n",
    "        setpoint = 1.0\n",
    "        state = 0.0\n",
    "        states = []\n",
    "        \n",
    "        controller = PrimalLogicController(KP=KP, KE=KE, lambda_val=lambda_val)\n",
    "        \n",
    "        for t in t_sim:\n",
    "            error = setpoint - state\n",
    "            u = controller.update(error, t, dt)\n",
    "            state += dt * (-state + u)\n",
    "            states.append(state)\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(t_sim, states, 'b-', linewidth=2, label='State')\n",
    "        plt.axhline(y=setpoint, color='r', linestyle='--', alpha=0.5, label='Setpoint')\n",
    "        plt.xlabel('Time (s)', fontsize=12)\n",
    "        plt.ylabel('State', fontsize=12)\n",
    "        plt.title(f'Step Response (KP={KP:.2f}, KE={KE:.2f}, λ={lambda_val:.4f})', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.ylim([0, 1.5])\n",
    "        plt.show()\n",
    "    \n",
    "    # Create interactive widget\n",
    "    interact(interactive_control,\n",
    "             KP=FloatSlider(min=0.1, max=5.0, step=0.1, value=2.0, description='KP'),\n",
    "             KE=FloatSlider(min=0.1, max=2.0, step=0.1, value=0.8, description='KE'),\n",
    "             lambda_val=FloatSlider(min=0.05, max=0.5, step=0.01, value=LAMBDA, description='λ'))\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"ipywidgets not available. Install with: pip install ipywidgets\")\n",
    "    print(\"For Colab, restart runtime after installation.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Lightfoot constant (λ = 0.16905 s⁻¹)** provides exponential memory decay\n",
    "2. **Donte constant (D = 149.9992314000)** ensures bounded convergence\n",
    "3. **Exponential weighting** prevents integral windup\n",
    "4. **Lipschitz continuity** guarantees stability\n",
    "5. **Parameter tuning** allows adaptation to different systems\n",
    "\n",
    "### Next Notebooks\n",
    "\n",
    "- **02_multi_language_comparison.ipynb**: Compare APL, Python, Prolog, and D implementations\n",
    "- **03_fixed_point_convergence.ipynb**: Deep dive into convergence analysis\n",
    "- **experiments/01_parameter_sweep_analysis.ipynb**: Systematic parameter exploration\n",
    "\n",
    "### References\n",
    "\n",
    "- [MotorHandPro Repository](https://github.com/STLNFTART/MotorHandPro)\n",
    "- `PRIMAL_LOGIC_FRAMEWORK.md` - Mathematical framework documentation\n",
    "- `README.md` - Project overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**License**: MIT License\n",
    "\n",
    "**Authors**: MotorHandPro Team\n",
    "\n",
    "**Contact**: See repository for details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
