{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-Accelerated Simulations\n",
    "\n",
    "[![Brev](https://img.shields.io/badge/Run%20on-Brev-orange)](https://brev.dev)\n",
    "\n",
    "**Optimized for Brev GPU instances**\n",
    "\n",
    "This notebook demonstrates GPU-accelerated large-scale simulations using CUDA/PyTorch.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- GPU with CUDA support (Tesla T4, A100, etc.)\n",
    "- PyTorch with CUDA\n",
    "- At least 8GB GPU memory recommended\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. GPU availability detection\n",
    "2. Vectorized simulation on GPU\n",
    "3. Batch parameter sweeps with GPU acceleration\n",
    "4. Performance benchmarking (CPU vs GPU)\n",
    "5. Large-scale mission simulations (millions of time steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and GPU Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"⚠ This notebook is optimized for Brev with dedicated GPU.\")\n",
    "    print(\"It will work on Colab but may be slower.\\n\")\n",
    "    !pip install -q torch torchvision\n",
    "else:\n",
    "    print(\"Running on Brev or local GPU environment\")\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"⚠ PyTorch not found. Installing...\")\n",
    "    !pip install -q torch torchvision\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Detection and Information\n",
    "print(\"=\"*70)\n",
    "print(\"GPU ENVIRONMENT CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ CUDA Available: YES\")\n",
    "    print(f\"  GPU Count:      {torch.cuda.device_count()}\")\n",
    "    print(f\"  Current Device: {torch.cuda.current_device()}\")\n",
    "    print(f\"  Device Name:    {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  CUDA Version:   {torch.version.cuda}\")\n",
    "    \n",
    "    # Memory info\n",
    "    mem_allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    mem_reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    print(f\"  Memory Allocated: {mem_allocated:.2f} GB\")\n",
    "    print(f\"  Memory Reserved:  {mem_reserved:.2f} GB\")\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "    USE_GPU = True\n",
    "else:\n",
    "    print(f\"✗ CUDA Available: NO\")\n",
    "    print(f\"  Falling back to CPU\")\n",
    "    device = torch.device('cpu')\n",
    "    USE_GPU = False\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU-Accelerated Primal Logic Simulation\n",
    "\n",
    "Implement the Primal Logic control law using PyTorch tensors for GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primal Logic constants\n",
    "D = 149.9992314000\n",
    "I3 = 6.4939394023\n",
    "S = D / I3\n",
    "LAMBDA = 0.16905\n",
    "\n",
    "def primal_logic_gpu(t_max=100.0, dt=0.01, KE=0.3, batch_size=1, device='cuda'):\n",
    "    \"\"\"\n",
    "    GPU-accelerated Primal Logic simulation with batch processing.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    t_max : float\n",
    "        Maximum simulation time\n",
    "    dt : float\n",
    "        Time step\n",
    "    KE : float or tensor\n",
    "        Error gain (can be array for batch)\n",
    "    batch_size : int\n",
    "        Number of parallel simulations\n",
    "    device : str\n",
    "        'cuda' or 'cpu'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Dictionary with tensors: t, psi, gamma, Ec\n",
    "    \"\"\"\n",
    "    # Time array\n",
    "    n = int(t_max / dt)\n",
    "    t = torch.arange(0, t_max, dt, device=device)\n",
    "    \n",
    "    # Convert KE to tensor if needed\n",
    "    if isinstance(KE, (int, float)):\n",
    "        KE = torch.full((batch_size,), KE, device=device)\n",
    "    else:\n",
    "        KE = torch.tensor(KE, device=device)\n",
    "    \n",
    "    # State tensors (batch_size x n)\n",
    "    psi = torch.zeros(batch_size, n, device=device)\n",
    "    gamma = torch.zeros(batch_size, n, device=device)\n",
    "    Ec = torch.zeros(batch_size, n, device=device)\n",
    "    \n",
    "    # Initial conditions\n",
    "    psi[:, 0] = 1.0\n",
    "    gamma[:, 0] = 0.01\n",
    "    \n",
    "    # Simulation loop\n",
    "    for i in range(1, n):\n",
    "        # Error dynamics\n",
    "        gamma[:, i] = gamma[:, i-1] * torch.exp(torch.tensor(-0.1 * dt, device=device))\n",
    "        \n",
    "        # Control law: dψ/dt = -λ·ψ + KE·error\n",
    "        dpsi_dt = -LAMBDA * psi[:, i-1] + KE * gamma[:, i]\n",
    "        psi[:, i] = psi[:, i-1] + dpsi_dt * dt\n",
    "        \n",
    "        # Integrate control energy\n",
    "        Ec[:, i] = Ec[:, i-1] + psi[:, i] * gamma[:, i] * dt\n",
    "    \n",
    "    return {\n",
    "        't': t,\n",
    "        'psi': psi,\n",
    "        'gamma': gamma,\n",
    "        'Ec': Ec\n",
    "    }\n",
    "\n",
    "print(\"✓ GPU simulation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmark: CPU vs GPU\n",
    "\n",
    "Compare execution time for CPU and GPU implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark parameters\n",
    "batch_sizes = [1, 10, 100, 1000]\n",
    "t_max = 100.0\n",
    "dt = 0.01\n",
    "\n",
    "results = {'batch_size': [], 'cpu_time': [], 'gpu_time': [], 'speedup': []}\n",
    "\n",
    "print(\"Running benchmarks...\\n\")\n",
    "print(f\"{'Batch Size':<12} {'CPU Time':<12} {'GPU Time':<12} {'Speedup':<12}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # CPU benchmark\n",
    "    start = time.time()\n",
    "    result_cpu = primal_logic_gpu(t_max, dt, KE=0.3, batch_size=batch_size, device='cpu')\n",
    "    cpu_time = time.time() - start\n",
    "    \n",
    "    # GPU benchmark (if available)\n",
    "    if USE_GPU:\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        result_gpu = primal_logic_gpu(t_max, dt, KE=0.3, batch_size=batch_size, device='cuda')\n",
    "        torch.cuda.synchronize()\n",
    "        gpu_time = time.time() - start\n",
    "        speedup = cpu_time / gpu_time\n",
    "    else:\n",
    "        gpu_time = None\n",
    "        speedup = None\n",
    "    \n",
    "    # Store results\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['cpu_time'].append(cpu_time)\n",
    "    results['gpu_time'].append(gpu_time if gpu_time else 0)\n",
    "    results['speedup'].append(speedup if speedup else 0)\n",
    "    \n",
    "    # Print\n",
    "    if USE_GPU:\n",
    "        print(f\"{batch_size:<12} {cpu_time:<12.4f} {gpu_time:<12.4f} {speedup:<12.2f}x\")\n",
    "    else:\n",
    "        print(f\"{batch_size:<12} {cpu_time:<12.4f} {'N/A':<12} {'N/A':<12}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"\\n✓ Benchmark complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize benchmark results\n",
    "if USE_GPU:\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('Execution Time Comparison', 'GPU Speedup Factor')\n",
    "    )\n",
    "    \n",
    "    # Time comparison\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=results['batch_size'], y=results['cpu_time'], name='CPU', \n",
    "                   mode='lines+markers', line=dict(color='blue', width=2)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=results['batch_size'], y=results['gpu_time'], name='GPU',\n",
    "                   mode='lines+markers', line=dict(color='green', width=2)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Speedup\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=results['batch_size'], y=results['speedup'], name='Speedup',\n",
    "                   mode='lines+markers', line=dict(color='red', width=2),\n",
    "                   fill='tozeroy'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Batch Size\", type=\"log\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Batch Size\", type=\"log\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Time (s)\", type=\"log\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Speedup (x)\", row=1, col=2)\n",
    "    \n",
    "    fig.update_layout(height=400, title_text=\"CPU vs GPU Performance\")\n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"\\nMax speedup: {max(results['speedup']):.2f}x at batch size {results['batch_size'][results['speedup'].index(max(results['speedup']))]}\")\n",
    "else:\n",
    "    print(\"GPU not available - skipping visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Massive Parameter Sweep\n",
    "\n",
    "Run thousands of simulations in parallel to explore the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "KE_values = torch.linspace(0.0, 1.0, 100, device=device)\n",
    "\n",
    "print(f\"Running {len(KE_values)} simulations in parallel...\")\n",
    "\n",
    "# Run batch simulation\n",
    "if USE_GPU:\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "start = time.time()\n",
    "results_sweep = primal_logic_gpu(t_max=50.0, dt=0.01, KE=KE_values, \n",
    "                                  batch_size=len(KE_values), device=device)\n",
    "\n",
    "if USE_GPU:\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "sweep_time = time.time() - start\n",
    "\n",
    "print(f\"✓ Completed {len(KE_values)} simulations in {sweep_time:.3f} seconds\")\n",
    "print(f\"  Average time per simulation: {sweep_time/len(KE_values)*1000:.2f} ms\")\n",
    "\n",
    "# Extract metrics\n",
    "max_psi = torch.max(torch.abs(results_sweep['psi']), dim=1).values.cpu().numpy()\n",
    "final_Ec = results_sweep['Ec'][:, -1].cpu().numpy()\n",
    "KE_np = KE_values.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter sweep results\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Max |ψ| vs KE', 'Final Ec vs KE')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=KE_np, y=max_psi, mode='lines', line=dict(color='blue', width=2)),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=KE_np, y=final_Ec, mode='lines', line=dict(color='green', width=2)),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"KE (Error Gain)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"KE (Error Gain)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Max |ψ|\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Final Ec\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=400, title_text=\"Parameter Sweep: KE Sensitivity Analysis\", showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "# Find optimal KE\n",
    "optimal_idx = np.argmin(max_psi)\n",
    "print(f\"\\nOptimal KE for minimum overshoot: {KE_np[optimal_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Parameter Space Exploration\n",
    "\n",
    "Visualize the relationship between KE, time, and control response in 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample parameter space for 3D visualization\n",
    "n_samples = 20\n",
    "KE_3d = torch.linspace(0.1, 0.8, n_samples, device=device)\n",
    "\n",
    "results_3d = primal_logic_gpu(t_max=30.0, dt=0.1, KE=KE_3d, \n",
    "                               batch_size=n_samples, device=device)\n",
    "\n",
    "# Convert to numpy for plotting\n",
    "t_3d = results_3d['t'].cpu().numpy()\n",
    "psi_3d = results_3d['psi'].cpu().numpy()\n",
    "KE_3d_np = KE_3d.cpu().numpy()\n",
    "\n",
    "# Create 3D surface\n",
    "fig = go.Figure(data=[go.Surface(\n",
    "    x=t_3d,\n",
    "    y=KE_3d_np,\n",
    "    z=psi_3d,\n",
    "    colorscale='Viridis',\n",
    "    hovertemplate='Time: %{x:.2f}s<br>KE: %{y:.3f}<br>ψ: %{z:.4f}<extra></extra>'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Control Response Surface',\n",
    "    scene=dict(\n",
    "        xaxis_title='Time (s)',\n",
    "        yaxis_title='KE',\n",
    "        zaxis_title='ψ(t)',\n",
    "        camera=dict(eye=dict(x=1.5, y=-1.5, z=1.2))\n",
    "    ),\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Memory Profiling\n",
    "\n",
    "Monitor GPU memory usage during simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU:\n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"GPU Memory Profile\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    mem_before = torch.cuda.memory_allocated(0) / 1e6\n",
    "    print(f\"Memory before: {mem_before:.2f} MB\")\n",
    "    \n",
    "    # Run large simulation\n",
    "    large_batch = 5000\n",
    "    print(f\"\\nRunning {large_batch} simulations...\")\n",
    "    \n",
    "    result_large = primal_logic_gpu(\n",
    "        t_max=10.0, dt=0.01, \n",
    "        KE=torch.rand(large_batch, device='cuda') * 0.5,\n",
    "        batch_size=large_batch,\n",
    "        device='cuda'\n",
    "    )\n",
    "    \n",
    "    mem_during = torch.cuda.memory_allocated(0) / 1e6\n",
    "    print(f\"Memory during: {mem_during:.2f} MB\")\n",
    "    print(f\"Memory used:   {mem_during - mem_before:.2f} MB\")\n",
    "    \n",
    "    # Clear results\n",
    "    del result_large\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    mem_after = torch.cuda.memory_allocated(0) / 1e6\n",
    "    print(f\"Memory after:  {mem_after:.2f} MB\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\n✓ Peak memory usage: {mem_during:.2f} MB for {large_batch} parallel simulations\")\n",
    "else:\n",
    "    print(\"GPU not available - skipping memory profiling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Save GPU-accelerated simulation results for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export parameter sweep results\n",
    "export_data = pd.DataFrame({\n",
    "    'KE': KE_np,\n",
    "    'max_psi': max_psi,\n",
    "    'final_Ec': final_Ec\n",
    "})\n",
    "\n",
    "output_file = 'gpu_parameter_sweep_results.csv'\n",
    "export_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"✓ Exported results to: {output_file}\")\n",
    "print(f\"  Records: {len(export_data)}\")\n",
    "print(f\"  Columns: {', '.join(export_data.columns)}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(export_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "✓ GPU detection and configuration  \n",
    "✓ GPU-accelerated Primal Logic simulations  \n",
    "✓ Performance comparison (CPU vs GPU)  \n",
    "✓ Massive parallel parameter sweeps  \n",
    "✓ 3D visualization of parameter space  \n",
    "✓ GPU memory profiling  \n",
    "\n",
    "**Performance Summary:**\n",
    "- GPU provides significant speedup for batch simulations\n",
    "- Optimal for parameter sweeps with hundreds to thousands of runs\n",
    "- Essential for real-time mission planning and optimization\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Scale to multi-GPU training\n",
    "- Implement distributed parameter optimization\n",
    "- Real-time mission trajectory optimization\n",
    "\n",
    "---\n",
    "\n",
    "**Patent Pending:** U.S. Provisional Patent Application No. 63/842,846  \n",
    "© 2025 Donte Lightfoot - The Phoney Express LLC / Locked In Safety"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
